Casal2
Call: ..\..\..\..\..\BuildSystem\bin\windows\release_cppad\casal2 -e -o params_est.out 
Date: Wed Jul 29 07:07:13 2020
Version: 2020-07-28 08:18:28 UTC (rev. 258dfb65)
Copyright (c) 2020 - 2020, NIWA (www.niwa.co.nz)
Environment: machine:niwa-computer, user:niwauser, os:Windows_NT, pid:12892

[WARNING] C:\workspace\casal2-windows-rlibrary\CASAL2\source\Estimates\Manager.cpp(line: 95): Estimates were removed because of matching lower and upper bounds. Originally had 127 estimates, now have 42
This is Ipopt version 3.11.9, running with linear solver mumps.
NOTE: Other linear solvers might be more efficient (see Ipopt documentation).

Starting derivative checker for first derivatives.


No errors detected by derivative checker.

Number of nonzeros in equality constraint Jacobian...:        0
Number of nonzeros in inequality constraint Jacobian.:       42
Number of nonzeros in Lagrangian Hessian.............:      903

Total number of variables............................:       42
                     variables with only lower bounds:        0
                variables with lower and upper bounds:       42
                     variables with only upper bounds:        0
Total number of equality constraints.................:        0
Total number of inequality constraints...............:        1
        inequality constraints with only lower bounds:        1
   inequality constraints with lower and upper bounds:        0
        inequality constraints with only upper bounds:        0

iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
   0  4.0792995e+02 0.00e+00 9.35e+01  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0 y
   1  3.3608455e+02 0.00e+00 2.69e+01  -1.0 2.46e-01   2.0 8.85e-01 9.90e-01f  1 Nhj 
   2  2.6976893e+02 0.00e+00 1.08e+01  -1.0 4.15e-01   1.5 9.90e-01 9.90e-01f  1 
   3  2.5366854e+02 0.00e+00 9.63e+02  -1.0 5.14e-01   1.0 9.90e-01 1.00e+00f  1 
   4  2.4688299e+02 0.00e+00 4.04e+00  -1.0 1.19e+00   0.6 1.00e+00 1.00e+00f  1 
   5  2.4142976e+02 0.00e+00 1.94e+06  -3.8 1.73e+00   0.1 8.06e-01 1.00e+00f  1 
   6  2.3985022e+02 0.00e+00 4.57e+05  -3.8 2.55e+03    -  7.64e-01 2.50e-01f  3 
   7  2.3937853e+02 0.00e+00 6.42e+02  -3.8 1.69e+02    -  9.99e-01 1.00e+00f  1 
   8  2.3936581e+02 0.00e+00 9.10e-02  -3.8 1.58e+01    -  1.00e+00 1.00e+00f  1 
   9  2.3936487e+02 0.00e+00 4.39e-02  -3.8 3.27e-01    -  1.00e+00 1.00e+00f  1 
iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
  10  2.3936468e+02 0.00e+00 1.50e-02  -3.8 9.39e-02    -  1.00e+00 1.00e+00f  1 
  11  2.3936464e+02 0.00e+00 2.86e-03  -3.8 2.27e-02    -  1.00e+00 1.00e+00f  1 
  12  2.3936462e+02 0.00e+00 6.82e-04  -5.7 2.54e-02    -  1.00e+00 1.00e+00f  1 
  13  2.3936461e+02 0.00e+00 1.48e-04  -5.7 2.13e-02    -  1.00e+00 1.00e+00f  1 
  14  2.3936461e+02 0.00e+00 5.00e-05  -5.7 1.65e-02    -  1.00e+00 1.00e+00f  1 
  15  2.3936461e+02 0.00e+00 2.10e-05  -5.7 1.09e-02    -  1.00e+00 1.00e+00f  1 
  16  2.3936461e+02 0.00e+00 3.51e-06  -5.7 8.86e-02    -  1.00e+00 1.56e-02f  7 
  17  2.3936461e+02 0.00e+00 1.73e+02  -8.6 7.95e-03    -  1.00e+00 6.25e-02f  5 
  18  2.3936461e+02 0.00e+00 1.73e+02  -8.6 1.96e+00    -  1.00e+00 1.53e-05f 17 
  19  2.3936461e+02 0.00e+00 1.73e+02  -8.6 2.65e+00    -  1.00e+00 3.81e-06f 19 
iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
  20  2.3936461e+02 0.00e+00 1.73e+02  -8.6 2.35e+00    -  1.00e+00 5.96e-08f 25 

Number of Iterations....: 20

                                   (scaled)                 (unscaled)
Objective...............:   2.3936460920734365e+02    2.3936460920734365e+02
Dual infeasibility......:   1.7272249763501767e+02    1.7272249763501767e+02
Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00
Complementarity.........:   5.0160230881020681e-09    5.0160230881020681e-09
Overall NLP error.......:   1.7272249763501767e+02    1.7272249763501767e+02


Number of objective function evaluations             = 115
Number of objective gradient evaluations             = 21
Number of equality constraint evaluations            = 0
Number of inequality constraint evaluations          = 115
Number of equality constraint Jacobian evaluations   = 0
Number of inequality constraint Jacobian evaluations = 21
Number of Lagrangian Hessian evaluations             = 20
Total CPU secs in IPOPT (w/o function evaluations)   =     38.259
Total CPU secs in NLP function evaluations           =    590.913

EXIT: Maximum Number of Iterations Exceeded.
*initialisation_partition[Init]
values {d}
category 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
male 167157.007101 145319.320727 126334.548237 109829.979925 95481.597542 83007.713152 72163.439030 62735.880016 54539.953947 47414.758122 41220.410453 35835.303293 31153.716036 27083.739599 23545.472066 20469.450047 17795.284972 118419.079758
female 157422.540827 128886.675403 105523.484814 86395.322190 70734.507199 57912.516347 47414.758122 38819.920624 31783.062847 26021.770980 21304.824151 17442.914721 14281.050705 11692.335399 9572.874566 7837.606803 6416.889720 28982.880671
*end

*partition[state1]
year: 2001
time_step: 1
values {d_r}
category 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
male 164198.045137 135077.250919 138852.835941 152154.640059 75866.706466 43114.860103 30495.957972 25947.691063 27600.791562 16990.152844 7826.546252 3771.096208 1918.539946 1102.855682 726.903442 529.051525 409.356653 4918.116881
female 154729.206548 123612.529191 130658.074817 148429.038821 75267.631912 42688.199106 30021.874613 25419.490868 26787.648607 16108.369112 7035.397277 3130.062474 1452.588837 746.260533 434.339658 281.229216 195.802764 1381.973033
*end

*partition[state1]
year: 2010
time_step: 1
values {d_r}
category 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
male 143040.081371 151517.835298 50182.209073 88062.073717 74462.274012 22574.953713 25670.830489 19875.246188 11354.898213 10820.415028 7744.505941 7381.759223 7855.199441 3881.279819 2202.859654 1557.758130 1325.331100 3360.368172
female 134808.834299 139126.243489 48070.485935 89538.574341 78213.346544 23679.038641 26185.637284 19205.674438 10193.485298 9043.903103 6113.226800 5535.641590 5570.479615 2601.954308 1400.872224 953.624278 790.662987 1752.230220
*end

*partition[state1]
year: 2019
time_step: 1
values {d_r}
category 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
male 143718.794006 120920.083652 55993.174485 53831.313606 51576.351756 34953.886369 46928.463271 11128.670040 15549.513486 9563.531260 8515.776229 2568.883269 4348.887889 3637.050724 1100.878743 1251.463939 968.828223 2801.870345
female 135454.166596 110593.452294 52465.363793 52240.579662 51295.316520 34842.532456 45958.517960 10638.910518 14387.908264 8485.387309 7219.010737 2077.706085 3351.413669 2656.062573 756.130693 804.330328 575.170227 1273.900742
*end

*estimate_value[estimated_values]
values {d}
process[Recruitment].b0 age_length[asMm0].linf age_length[asMm0].cv_last selectivity[potSurveySel_male].a50 selectivity[potSurveySel_male].ato95 selectivity[potSurveySel_female].a50 selectivity[potSurveySel_female].ato95 process[Recruitment].ycs_values{1980} process[Recruitment].ycs_values{1981} process[Recruitment].ycs_values{1982} process[Recruitment].ycs_values{1983} process[Recruitment].ycs_values{1984} process[Recruitment].ycs_values{1985} process[Recruitment].ycs_values{1986} process[Recruitment].ycs_values{1987} process[Recruitment].ycs_values{1988} process[Recruitment].ycs_values{1989} process[Recruitment].ycs_values{1990} process[Recruitment].ycs_values{1991} process[Recruitment].ycs_values{1992} process[Recruitment].ycs_values{1993} process[Recruitment].ycs_values{1994} process[Recruitment].ycs_values{1995} process[Recruitment].ycs_values{1996} process[Recruitment].ycs_values{1997} process[Recruitment].ycs_values{1998} process[Recruitment].ycs_values{1999} process[Recruitment].ycs_values{2000} process[Recruitment].ycs_values{2001} process[Recruitment].ycs_values{2002} process[Recruitment].ycs_values{2003} process[Recruitment].ycs_values{2004} process[Recruitment].ycs_values{2005} process[Recruitment].ycs_values{2006} process[Recruitment].ycs_values{2007} process[Recruitment].ycs_values{2008} process[Recruitment].ycs_values{2009} process[Recruitment].ycs_values{2010} process[Recruitment].ycs_values{2011} process[Recruitment].ycs_values{2012} process[Recruitment].ycs_values{2013} process[Recruitment].ycs_values{2014} 
14210.7 47.3135 0.0614247 3.04015 0.19197 3.3151 1.02651 0.23512 0.232089 0.233169 0.241993 0.264699 0.314953 0.416347 0.601999 0.90255 1.37986 1.55858 1.02851 0.8478 0.847081 1.06575 1.5481 1.05337 0.802935 0.816576 0.628936 0.802337 0.745992 0.466754 1.09063 0.917104 0.38256 0.901903 0.720451 0.834791 0.426064 1.28707 0.691099 0.736775 0.561903 0.439778 
std_dev {d}
process[Recruitment].b0 age_length[asMm0].linf age_length[asMm0].cv_last selectivity[potSurveySel_male].a50 selectivity[potSurveySel_male].ato95 selectivity[potSurveySel_female].a50 selectivity[potSurveySel_female].ato95 process[Recruitment].ycs_values{1980} process[Recruitment].ycs_values{1981} process[Recruitment].ycs_values{1982} process[Recruitment].ycs_values{1983} process[Recruitment].ycs_values{1984} process[Recruitment].ycs_values{1985} process[Recruitment].ycs_values{1986} process[Recruitment].ycs_values{1987} process[Recruitment].ycs_values{1988} process[Recruitment].ycs_values{1989} process[Recruitment].ycs_values{1990} process[Recruitment].ycs_values{1991} process[Recruitment].ycs_values{1992} process[Recruitment].ycs_values{1993} process[Recruitment].ycs_values{1994} process[Recruitment].ycs_values{1995} process[Recruitment].ycs_values{1996} process[Recruitment].ycs_values{1997} process[Recruitment].ycs_values{1998} process[Recruitment].ycs_values{1999} process[Recruitment].ycs_values{2000} process[Recruitment].ycs_values{2001} process[Recruitment].ycs_values{2002} process[Recruitment].ycs_values{2003} process[Recruitment].ycs_values{2004} process[Recruitment].ycs_values{2005} process[Recruitment].ycs_values{2006} process[Recruitment].ycs_values{2007} process[Recruitment].ycs_values{2008} process[Recruitment].ycs_values{2009} process[Recruitment].ycs_values{2010} process[Recruitment].ycs_values{2011} process[Recruitment].ycs_values{2012} process[Recruitment].ycs_values{2013} process[Recruitment].ycs_values{2014} 
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 
*end

*objective_function[obj_fun]
values {v}
observation->potSurvey-2010 -1.04213
observation->potSurvey-2014 -1.02191
observation->potSurvey-2018 -1.1966
observation->potCPUE-1990 -0.159256
observation->potCPUE-1991 1.10236
observation->potCPUE-1992 -1.05339
observation->potCPUE-1993 -1.99001
observation->potCPUE-1994 -2.14319
observation->potCPUE-1995 -2.08542
observation->potCPUE-1996 -2.18618
observation->potCPUE-1997 -1.95939
observation->potCPUE-1998 -2.1571
observation->potCPUE-1999 -1.68858
observation->potCPUE-2000 -2.19162
observation->potCPUE-2001 -2.16418
observation->potCPUE-2002 -2.04222
observation->potCPUE-2003 -2.17258
observation->potCPUE-2004 -2.19202
observation->potCPUE-2005 -1.91112
observation->potCPUE-2006 -1.79719
observation->potCPUE-2007 -2.17704
observation->potCPUE-2008 -2.18868
observation->potCPUE-2009 -2.05121
observation->potCPUE-2010 -2.1244
observation->potCPUE-2011 -1.86529
observation->potCPUE-2012 -1.75578
observation->potCPUE-2013 -1.76945
observation->potCPUE-2014 -1.03421
observation->potCPUE-2015 -1.85139
observation->potCPUE-2016 -2.09207
observation->potCPUE-2017 -2.10498
observation->potCPUE-2018 -1.34659
observation->AFpotSurvey-2010 39.7536
observation->AFpotSurvey-2014 39.383
observation->AFpotSurvey-2018 39.1361
observation->AFpotFishing-2018 47.1456
observation->AFpotFishing-2019 7.8874
observation->lgobookLF 54.8618
observation->rec.catch.LF 68.6107
prior->B0->process[Recruitment].b0 9.56175
prior->male_Linf->age_length[asMm0].linf 0
prior->male_cv2->age_length[asMm0].cv_last 0
prior->potSurvey_mA50->selectivity[potSurveySel_male].a50 0
prior->potSurvey_mAto95->selectivity[potSurveySel_male].ato95 0
prior->potSurvey_fA50->selectivity[potSurveySel_female].a50 0
prior->potSurvey_fAto95->selectivity[potSurveySel_female].ato95 0
prior->YCS->process[Recruitment].ycs_values{1980} 1.27479
prior->YCS->process[Recruitment].ycs_values{1981} 1.31668
prior->YCS->process[Recruitment].ycs_values{1982} 1.30163
prior->YCS->process[Recruitment].ycs_values{1983} 1.18371
prior->YCS->process[Recruitment].ycs_values{1984} 0.91747
prior->YCS->process[Recruitment].ycs_values{1985} 0.475939
prior->YCS->process[Recruitment].ycs_values{1986} -0.0274179
prior->YCS->process[Recruitment].ycs_values{1987} -0.304003
prior->YCS->process[Recruitment].ycs_values{1988} -0.0982667
prior->YCS->process[Recruitment].ycs_values{1989} 0.689985
prior->YCS->process[Recruitment].ycs_values{1990} 1.02434
prior->YCS->process[Recruitment].ycs_values{1991} 0.0818814
prior->YCS->process[Recruitment].ycs_values{1992} -0.1649
prior->YCS->process[Recruitment].ycs_values{1993} -0.165717
prior->YCS->process[Recruitment].ycs_values{1994} 0.140554
prior->YCS->process[Recruitment].ycs_values{1995} 1.00456
prior->YCS->process[Recruitment].ycs_values{1996} 0.120817
prior->YCS->process[Recruitment].ycs_values{1997} -0.212454
prior->YCS->process[Recruitment].ycs_values{1998} -0.198748
prior->YCS->process[Recruitment].ycs_values{1999} -0.307475
prior->YCS->process[Recruitment].ycs_values{2000} -0.213039
prior->YCS->process[Recruitment].ycs_values{2001} -0.261488
prior->YCS->process[Recruitment].ycs_values{2002} -0.160427
prior->YCS->process[Recruitment].ycs_values{2003} 0.180808
prior->YCS->process[Recruitment].ycs_values{2004} -0.0791895
prior->YCS->process[Recruitment].ycs_values{2005} 0.0984584
prior->YCS->process[Recruitment].ycs_values{2006} -0.0991019
prior->YCS->process[Recruitment].ycs_values{2007} -0.278569
prior->YCS->process[Recruitment].ycs_values{2008} -0.179404
prior->YCS->process[Recruitment].ycs_values{2009} -0.0576917
prior->YCS->process[Recruitment].ycs_values{2010} 0.520545
prior->YCS->process[Recruitment].ycs_values{2011} -0.293795
prior->YCS->process[Recruitment].ycs_values{2012} -0.268037
prior->YCS->process[Recruitment].ycs_values{2013} -0.285905
prior->YCS->process[Recruitment].ycs_values{2014} -0.0964398
additional_prior->PrSurveyQ -11.3684
additional_prior->PrCPUEQ -8.33364
additional_prior->YCS_average_1 0.559302
total_negloglike 239.365
*end

*derived_quantity[DerivedQuantities]
SSB {L} 
type: biomass 
initialisation_phase[1]: 14210.7 
values {v}
1900 14210.7
1901 14176.3
1902 14127.1
1903 14066.5
1904 13995.9
1905 13916.2
1906 13828.6
1907 13733.7
1908 13632.5
1909 13525.4
1910 13413.2
1911 13296.5
1912 13176
1913 13052.2
1914 12925.3
1915 12795.7
1916 12663.5
1917 12529
1918 12392.4
1919 12253.7
1920 12113.2
1921 11970.7
1922 11826.4
1923 11680.3
1924 11532.4
1925 11382.9
1926 11231.7
1927 11079.1
1928 10924.9
1929 10769.3
1930 10612.2
1931 10453.6
1932 10294.5
1933 10106.4
1934 9784.78
1935 9897.61
1936 10153
1937 10386
1938 10644.3
1939 10867.4
1940 11120.7
1941 11315.5
1942 11482.8
1943 11595
1944 11496.9
1945 11335.4
1946 11130.6
1947 10787.6
1948 10508.2
1949 10195.1
1950 9861.49
1951 9627.45
1952 9395.79
1953 9448.27
1954 9533.87
1955 9604.49
1956 9527.06
1957 9488.28
1958 9477.94
1959 9503.35
1960 9385.49
1961 9354.03
1962 9282.92
1963 9231.24
1964 9284.81
1965 9440.91
1966 9567.21
1967 9591.73
1968 9624.29
1969 9732.29
1970 9789.03
1971 9866.35
1972 10052.8
1973 10023
1974 10015.2
1975 10161.8
1976 10299.8
1977 10458.3
1978 10610.4
1979 10783.2
1980 10808.2
1981 10767.1
1982 10801
1983 10609.2
1984 10173.9
1985 9444.32
1986 8632.86
1987 7800.88
1988 6941.8
1989 6247.72
1990 5566.29
1991 4910.34
1992 4728.87
1993 4854.66
1994 5158.05
1995 5490.06
1996 5643.86
1997 5682.86
1998 5905.14
1999 6179.33
2000 6562.89
2001 6664.39
2002 6589.19
2003 6496.68
2004 6408.89
2005 6125.61
2006 5749.6
2007 5516.64
2008 5327.79
2009 5143.84
2010 4950.47
2011 4826.85
2012 4745.14
2013 4689.55
2014 4656.53
2015 4782.27
2016 4825.22
2017 4692.22
2018 4623.37
2019 4475.33
end {L}
*end

*process[Recruitment]
age: 3
b0: 14210.7
b0_initialisation_phase: 
categories: male female
label: Recruitment
proportions: 0.5 0.5
r0: 384553
ssb: SSB
ssb_offset: 3
standardise_ycs_years: 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014
steepness: 0.75
type: recruitment_beverton_holt
ycs_years: 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 
standardised_ycs: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0.316196 0.31212 0.313573 0.325439 0.355975 0.423558 0.559915 0.809585 1.21377 1.85567 2.09603 1.38316 1.14015 1.13918 1.43326 2.08193 1.4166 1.07981 1.09815 0.845811 1.07901 1.00323 0.627704 1.46671 1.23335 0.514478 1.21291 0.968884 1.12265 0.572984 1.73088 0.929409 0.990836 0.755664 0.591426 1 1 
ycs_values: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0.23512 0.232089 0.233169 0.241993 0.264699 0.314953 0.416347 0.601999 0.90255 1.37986 1.55858 1.02851 0.8478 0.847081 1.06575 1.5481 1.05337 0.802935 0.816576 0.628936 0.802337 0.745992 0.466754 1.09063 0.917104 0.38256 0.901903 0.720451 0.834791 0.426064 1.28707 0.691099 0.736775 0.561903 0.439778 1 1 
true_ycs: 1 1 1 1 0.999798 0.999507 0.999147 0.998723 0.99824 0.997703 0.997114 0.996478 0.995796 0.99507 0.994303 0.993499 0.992658 0.991781 0.990869 0.989922 0.988938 0.98792 0.986866 0.985776 0.984646 0.983477 0.982267 0.981014 0.979718 0.978376 0.976987 0.975549 0.974061 0.972519 0.970921 0.969273 0.967265 0.963676 0.964958 0.967769 0.970226 0.972838 0.975004 0.977369 0.979124 0.980588 0.981548 0.980709 0.9793 0.977459 0.974239 0.971476 0.968221 0.964551 0.961842 0.959045 0.959689 0.960727 0.96157 0.960645 0.960176 0.960051 0.960359 0.958917 0.958527 0.957637 0.956983 0.957661 0.959599 0.961126 0.961419 0.961805 0.96307 0.963724 0.964606 0.966682 0.966355 0.966269 0.967864 0.969329 0.970969 0.972502 0.974196 0.308113 0.304017 0.305535 0.316486 0.344581 0.406463 0.531308 0.757703 1.11636 1.6775 1.85585 1.19461 0.976913 0.981541 1.25038 1.83856 1.25753 0.959787 0.982946 0.763155 0.9835 0.916728 0.57252 1.33465 1.11975 0.463498 1.08041 0.856411 0.985698 0.499599 1.49746 0.799831 0.849604 0.646314 0.505069 0.858889 0.860518 
Recruits: 384553 384553 384553 384553 384475 384363 384225 384061 383876 383669 383443 383198 382936 382657 382362 382053 381729 381392 381041 380677 380299 379907 379502 379083 378648 378199 377733 377252 376753 376237 375703 375150 374578 373985 373370 372736 371964 370584 371077 372158 373103 374107 374940 375850 376525 377088 377457 377134 376592 375885 374646 373584 372332 370920 369879 368803 369051 369450 369774 369418 369238 369190 369308 368754 368604 368262 368010 368271 369016 369604 369716 369865 370351 370603 370942 371740 371614 371581 372195 372758 373389 373978 374629 118486 116911 117494 121705 132510 156307 204316 291377 429300 645088 713672 459391 375674 377454 480838 707024 483586 369089 377994 293473 378207 352530 220164 513243 430605 178239 415475 329335 379053 192122 575851 307577 326718 248542 194225 330288 330915 
SSB: 14210.7 14210.7 14210.7 14210.7 14176.3 14127.1 14066.5 13995.9 13916.2 13828.6 13733.7 13632.5 13525.4 13413.2 13296.5 13176 13052.2 12925.3 12795.7 12663.5 12529 12392.4 12253.7 12113.2 11970.7 11826.4 11680.3 11532.4 11382.9 11231.7 11079.1 10924.9 10769.3 10612.2 10453.6 10294.5 10106.4 9784.78 9897.61 10153 10386 10644.3 10867.4 11120.7 11315.5 11482.8 11595 11496.9 11335.4 11130.6 10787.6 10508.2 10195.1 9861.49 9627.45 9395.79 9448.27 9533.87 9604.49 9527.06 9488.28 9477.94 9503.35 9385.49 9354.03 9282.92 9231.24 9284.81 9440.91 9567.21 9591.73 9624.29 9732.29 9789.03 9866.35 10052.8 10023 10015.2 10161.8 10299.8 10458.3 10610.4 10783.2 10808.2 10767.1 10801 10609.2 10173.9 9444.32 8632.86 7800.88 6941.8 6247.72 5566.29 4910.34 4728.87 4854.66 5158.05 5490.06 5643.86 5682.86 5905.14 6179.33 6562.89 6664.39 6589.19 6496.68 6408.89 6125.61 5749.6 5516.64 5327.79 5143.84 4950.47 4826.85 4745.14 4689.55 4656.53 4782.27 4825.22 
*end

*process[Mortality]
categories: male female
label: Fishing
m: 0.14 0.2
relative_m_by_age: One One
time_step_ratio: 1
type: mortality_instantaneous_retained
year: 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 
fishing_pressure[FishingLine]: 0 0.00104471 0.00209521 0.00315454 0.0042174 0.00529412 0.0063787 0.00748918 0.00860306 0.00974629 0.0109042 0.0120864 0.0132944 0.0145206 0.015775 0.01705 0.0183648 0.0197031 0.0210664 0.0224648 0.0239003 0.0253746 0.0268807 0.0284298 0.0300243 0.0316568 0.0333394 0.0350647 0.0368553 0.0386945 0.0405846 0.0425399 0.044422 0.0495434 0.0666552 0.0180793 0.00724275 0.0102229 0.00561147 0.00787308 0.00236785 0.0069178 0.00734596 0.0111168 0.0302444 0.0346421 0.0396096 0.0552522 0.0506369 0.0584453 0.0643503 0.0571042 0.0615555 0.03024 0.0304441 0.0321301 0.0489848 0.0428275 0.0402341 0.0363099 0.0538081 0.042407 0.048896 0.0466289 0.0345025 0.0229419 0.0267196 0.0371949 0.0342888 0.0249125 0.0309539 0.0271351 0.0139795 0.0386031 0.0331289 0.0157113 0.0175784 0.0140272 0.0137329 0.0100839 0.0217005 0.0222857 0.0132498 0.0193239 0.0179555 0.014994 0.00727785 0.00033652 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
actual_catch[FishingLine]: 0 15.4661 30.9379 46.4211 61.804 77.2082 92.52 107.977 123.23 138.633 153.955 169.315 184.715 200.039 215.406 230.701 246.161 261.551 276.875 292.251 307.682 323.171 338.601 354.093 369.652 385.159 400.738 416.271 432.003 447.695 463.352 479.098 493.355 542.467 717.179 188.41 76.7579 111.198 62.3464 89.5305 27.4364 81.9159 88.329 135.443 371.523 421.036 474.573 650.278 577.69 651.052 696.558 599.109 632.468 303.823 308.836 328.977 505.048 437.338 409.49 369.32 548.996 426.808 491.179 464.822 342.28 229.192 271.541 382.574 352.995 257.267 323.304 284.75 147.812 415.937 354.916 168.219 191.145 154.449 153.419 114.16 249.437 256.224 151.653 221.702 200.124 157.254 69.715 2.92354 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
retained_catch[FishingLine]: 0 13.2 26.4 39.6 52.7 65.8 78.8 91.9 104.8 117.8 130.7 143.6 156.5 169.3 182.1 194.8 207.6 220.3 232.9 245.5 258.1 270.7 283.2 295.7 308.2 320.6 333 345.3 357.7 370 382.2 394.4 405.3 444.7 586.5 153.5 62.5 90.7 51 73.5 22.6 67.7 73.2 112.5 309.1 350.3 394.4 539.3 477.4 536 571 488.8 513.9 245.9 249.7 266.1 409 354.2 331.6 299 444.5 345.2 397 375.3 276.1 184.9 219.4 309.7 286.1 208.7 262.6 231.5 120.3 339.2 289.6 137.3 156.2 126.4 125.8 93.8 205.4 211.2 125 183 167.6 135.1 61.2 2.6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
actual_retained_catch[FishingLine]: 0 13.2 26.4 39.6 52.7 65.8 78.8 91.9 104.8 117.8 130.7 143.6 156.5 169.3 182.1 194.8 207.6 220.3 232.9 245.5 258.1 270.7 283.2 295.7 308.2 320.6 333 345.3 357.7 370 382.2 394.4 405.3 444.7 586.5 153.5 62.5 90.7 51 73.5 22.6 67.7 73.2 112.5 309.1 350.3 394.4 539.3 477.4 536 571 488.8 513.9 245.9 249.7 266.1 409 354.2 331.6 299 444.5 345.2 397 375.3 276.1 184.9 219.4 309.7 286.1 208.7 262.6 231.5 120.3 339.2 289.6 137.3 156.2 126.4 125.8 93.8 205.4 211.2 125 183 167.6 135.1 61.2 2.6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
discards[FishingLine]: 0 2.26611 4.53794 6.82113 9.10399 11.4082 13.72 16.077 18.4298 20.8334 23.2552 25.715 28.2151 30.7389 33.3063 35.9006 38.5607 41.2514 43.9745 46.7506 49.5819 52.4707 55.4005 58.3932 61.4516 64.5588 67.7376 70.9709 74.3026 77.6953 81.152 84.6978 88.0553 97.7673 130.679 34.9101 14.2579 20.4978 11.3464 16.0305 4.83644 14.2159 15.129 22.9432 62.4226 70.7359 80.1734 110.978 100.29 115.052 125.558 110.309 118.568 57.9231 59.1364 62.8765 96.0484 83.1383 77.8902 70.3195 104.496 81.6082 94.1785 89.5223 66.1804 44.2922 52.1414 72.8738 66.8951 48.5669 60.7039 53.25 27.5116 76.7368 65.3158 30.9193 34.9453 28.0489 27.6187 20.3599 44.0374 45.0236 26.6532 38.7024 32.5238 22.1538 8.51497 0.323539 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
discards_dead[FishingLine]: 0 2.26611 4.53794 6.82113 9.10399 11.4082 13.72 16.077 18.4298 20.8334 23.2552 25.715 28.2151 30.7389 33.3063 35.9006 38.5607 41.2514 43.9745 46.7506 49.5819 52.4707 55.4005 58.3932 61.4516 64.5588 67.7376 70.9709 74.3026 77.6953 81.152 84.6978 88.0553 97.7673 130.679 34.9101 14.2579 20.4978 11.3464 16.0305 4.83644 14.2159 15.129 22.9432 62.4226 70.7359 80.1734 110.978 100.29 115.052 125.558 110.309 118.568 57.9231 59.1364 62.8765 96.0484 83.1383 77.8902 70.3195 104.496 81.6082 94.1785 89.5223 66.1804 44.2922 52.1414 72.8738 66.8951 48.5669 60.7039 53.25 27.5116 76.7368 65.3158 30.9193 34.9453 28.0489 27.6187 20.3599 44.0374 45.0236 26.6532 38.7024 32.5238 22.1538 8.51497 0.323539 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
fishing_pressure[FishingPot]: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.0031954 0.00763783 0.00815487 0.019779 0.0305025 0.0453429 0.0500223 0.0603175 0.0759481 0.0703327 0.105001 0.160929 0.145479 0.182879 0.212812 0.207036 0.19954 0.185004 0.148698 0.161429 0.141505 0.166309 0.15648 0.135411 0.12374 0.154452 0.178167 0.170356 0.184225 0.186426 0.181274 0.178657 0.169853 0.17458 0.180287 0.156235 0.161319 0.172356 0.132994 0.155773 
actual_catch[FishingPot]: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 33.4751 80.0636 85.0872 207.146 313.612 444.903 453.004 497.808 564.624 463.72 623.141 849.191 676.2 827.143 998.184 1040.74 1064.45 1008.79 815.018 923.296 853.573 1067.94 1011.9 864.631 777.9 959.034 1055.17 938.066 979.709 962.035 894.465 851.169 790.443 802.1 811.264 705.368 750.138 806.158 602.789 694.892 
retained_catch[FishingPot]: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 29.2 69.9 74.3 180.9 275.1 396.3 409.9 455.5 519.3 425.7 566.9 756.2 576.6 660.5 738.2 739.3 767.9 749.9 617.8 698.4 636.7 807.7 782.9 683 625.3 775.7 856.5 759.6 773.7 751.8 704.7 665.7 614.4 623.9 631 535.6 572.2 623.5 475.2 557 
actual_retained_catch[FishingPot]: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 29.2 69.9 74.3 180.9 275.1 396.3 409.9 455.5 519.3 425.7 566.9 756.2 576.6 660.5 738.2 739.3 767.9 749.9 617.8 698.4 636.7 807.7 782.9 683 625.3 775.7 856.5 759.6 773.7 751.8 704.7 665.7 614.4 623.9 631 535.6 572.2 623.5 475.2 557 
discards[FishingPot]: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4.27508 10.1636 10.7872 26.2458 38.5115 48.6035 43.1037 42.3081 45.3244 38.0202 56.2415 92.9909 99.5998 166.643 259.984 301.444 296.55 258.894 197.218 224.896 216.873 260.24 229.005 181.631 152.6 183.334 198.668 178.466 206.009 210.235 189.765 185.469 176.043 178.2 180.264 169.768 177.938 182.658 127.589 137.892 
discards_dead[FishingPot]: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4.27508 10.1636 10.7872 26.2458 38.5115 48.6035 43.1037 42.3081 45.3244 38.0202 56.2415 92.9909 99.5998 166.643 259.984 301.444 296.55 258.894 197.218 224.896 216.873 260.24 229.005 181.631 152.6 183.334 198.668 178.466 206.009 210.235 189.765 185.469 176.043 178.2 180.264 169.768 177.938 182.658 127.589 137.892 
fishing_pressure[Recreation]: 0 0.0018661 0.00192034 0.00196859 0.0020272 0.0020797 0.00213424 0.0021992 0.00225787 0.00232707 0.00238989 0.00245477 0.00247796 0.00252001 0.00255466 0.00259969 0.00263715 0.00268523 0.00272561 0.00277687 0.00282029 0.00288461 0.00295107 0.00301975 0.00309077 0.00316425 0.00324028 0.00330852 0.00337924 0.0034418 0.00351778 0.00359669 0.00371281 0.00383301 0.00398226 0.00419015 0.00418408 0.00406965 0.00396036 0.00385373 0.00376735 0.00366166 0.00357357 0.00349679 0.00344102 0.0034465 0.00347934 0.00362427 0.00383057 0.00403032 0.00424994 0.00449358 0.00466029 0.00482624 0.00483758 0.00484184 0.00485219 0.00503647 0.00520503 0.00536097 0.00548238 0.0057173 0.00583492 0.00600317 0.00614275 0.00621567 0.0062015 0.00616429 0.00618734 0.00621097 0.00616872 0.00618182 0.00615164 0.00605855 0.00611257 0.00614932 0.00607008 0.00598464 0.00588551 0.00579417 0.00568136 0.00567293 0.00566514 0.00561054 0.0056577 0.00581103 0.00616581 0.00665136 0.00732783 0.00822464 0.00917793 0.0105123 0.0123327 0.0134825 0.0139718 0.0135227 0.012177 0.0114005 0.0109195 0.0102902 0.00976493 0.00912522 0.00889886 0.00887933 0.00886849 0.00889978 0.00923477 0.00995726 0.0105705 0.0110775 0.0115871 0.0122129 0.012602 0.0127788 0.0131184 0.0134681 0.0131189 0.0129321 0.0132016 0.0131172 
actual_catch[Recreation]: 0 26.0748 26.7618 27.3377 28.0305 28.6125 29.1972 29.8986 30.4885 31.1952 31.7902 32.3878 32.4152 32.6732 32.8184 33.0799 33.2282 33.4932 33.6448 33.9132 34.0681 34.4557 34.8457 35.238 35.6328 36.0301 36.4301 36.7163 37.005 37.1794 37.4732 37.7699 38.422 39.0784 39.8642 40.5762 41.1963 41.1495 40.9416 40.8237 40.7093 40.4797 40.1524 39.8412 39.5509 39.1948 38.992 39.8701 40.7999 41.8632 42.835 43.8291 44.4557 44.9603 45.4871 45.9574 46.3942 47.6959 49.1258 50.5619 51.8688 53.3397 54.3216 55.4412 56.4394 57.5127 58.4019 58.7905 59.0856 59.5131 59.8075 60.2349 60.4177 60.6762 60.8789 61.2146 61.3927 61.3206 61.2254 61.1227 60.8951 60.8391 60.4827 60.0534 59.1318 57.6459 56.292 55.3161 54.9858 54.9126 55.4682 57.0943 60.1567 65.502 71.3577 73.4823 69.2104 65.6478 63.3091 62.7373 62.6643 61.7004 60.1313 58.9139 57.8913 57.3647 56.7272 57.3694 59.1266 59.8383 59.9201 61.1168 61.6813 61.6071 62.4882 64.4403 64.2683 63.2571 62.2102 60.9746 
retained_catch[Recreation]: 0 22.9 23.5 24 24.6 25.1 25.6 26.2 26.7 27.3 27.8 28.3 28.3 28.5 28.6 28.8 28.9 29.1 29.2 29.4 29.5 29.8 30.1 30.4 30.7 31 31.3 31.5 31.7 31.8 32 32.2 32.7 33.2 33.8 34.3 34.8 34.8 34.7 34.7 34.7 34.6 34.4 34.2 34 33.7 33.5 34.2 34.9 35.7 36.4 37.1 37.5 37.8 38.2 38.6 39 40.1 41.3 42.5 43.6 44.8 45.6 46.5 47.3 48.2 49 49.4 49.7 50.1 50.4 50.8 51 51.3 51.5 51.8 52 52 52 52 51.9 51.9 51.6 51.3 51 50.7 50.4 50.1 50 49.7 49.5 49.4 49 49 49 48.9 47.3 46.5 45.6 44.8 44.3 44.6 44.7 44.9 44.9 44.8 44.5 44.5 44.6 44.9 45.2 45.7 45.8 45.9 46.1 46.5 46.8 47 47.3 47 
actual_retained_catch[Recreation]: 0 22.9 23.5 24 24.6 25.1 25.6 26.2 26.7 27.3 27.8 28.3 28.3 28.5 28.6 28.8 28.9 29.1 29.2 29.4 29.5 29.8 30.1 30.4 30.7 31 31.3 31.5 31.7 31.8 32 32.2 32.7 33.2 33.8 34.3 34.8 34.8 34.7 34.7 34.7 34.6 34.4 34.2 34 33.7 33.5 34.2 34.9 35.7 36.4 37.1 37.5 37.8 38.2 38.6 39 40.1 41.3 42.5 43.6 44.8 45.6 46.5 47.3 48.2 49 49.4 49.7 50.1 50.4 50.8 51 51.3 51.5 51.8 52 52 52 52 51.9 51.9 51.6 51.3 51 50.7 50.4 50.1 50 49.7 49.5 49.4 49 49 49 48.9 47.3 46.5 45.6 44.8 44.3 44.6 44.7 44.9 44.9 44.8 44.5 44.5 44.6 44.9 45.2 45.7 45.8 45.9 46.1 46.5 46.8 47 47.3 47 
discards[Recreation]: 0 3.17483 3.26177 3.33766 3.43053 3.51246 3.59716 3.69857 3.7885 3.89521 3.99025 4.0878 4.11516 4.17324 4.21837 4.27991 4.32825 4.39321 4.44476 4.51322 4.56805 4.65575 4.74572 4.83804 4.93281 5.03013 5.13011 5.21629 5.30497 5.37935 5.47318 5.56986 5.72204 5.87844 6.06417 6.27623 6.39632 6.34951 6.24161 6.12368 6.00929 5.87971 5.75236 5.64123 5.55091 5.49482 5.49204 5.67011 5.89987 6.16316 6.43499 6.72909 6.95565 7.16028 7.28707 7.35737 7.39421 7.59592 7.82575 8.06193 8.26877 8.53967 8.7216 8.94124 9.1394 9.31274 9.40192 9.3905 9.38558 9.41312 9.40754 9.43488 9.41774 9.37622 9.3789 9.41464 9.39269 9.32061 9.22536 9.1227 8.99511 8.93914 8.88267 8.75335 8.13182 6.94588 5.89202 5.21608 4.98582 5.2126 5.96816 7.69429 11.1567 16.502 22.3577 24.5823 21.9104 19.1478 17.7091 17.9373 18.3643 17.1004 15.4313 14.0139 12.9913 12.5647 12.2272 12.8694 14.5266 14.9383 14.7201 15.4168 15.8813 15.7071 16.3882 17.9403 17.4683 16.2571 14.9102 13.9746 
discards_dead[Recreation]: 0 3.17483 3.26177 3.33766 3.43053 3.51246 3.59716 3.69857 3.7885 3.89521 3.99025 4.0878 4.11516 4.17324 4.21837 4.27991 4.32825 4.39321 4.44476 4.51322 4.56805 4.65575 4.74572 4.83804 4.93281 5.03013 5.13011 5.21629 5.30497 5.37935 5.47318 5.56986 5.72204 5.87844 6.06417 6.27623 6.39632 6.34951 6.24161 6.12368 6.00929 5.87971 5.75236 5.64123 5.55091 5.49482 5.49204 5.67011 5.89987 6.16316 6.43499 6.72909 6.95565 7.16028 7.28707 7.35737 7.39421 7.59592 7.82575 8.06193 8.26877 8.53967 8.7216 8.94124 9.1394 9.31274 9.40192 9.3905 9.38558 9.41312 9.40754 9.43488 9.41774 9.37622 9.3789 9.41464 9.39269 9.32061 9.22536 9.1227 8.99511 8.93914 8.88267 8.75335 8.13182 6.94588 5.89202 5.21608 4.98582 5.2126 5.96816 7.69429 11.1567 16.502 22.3577 24.5823 21.9104 19.1478 17.7091 17.9373 18.3643 17.1004 15.4313 14.0139 12.9913 12.5647 12.2272 12.8694 14.5266 14.9383 14.7201 15.4168 15.8813 15.7071 16.3882 17.9403 17.4683 16.2571 14.9102 13.9746 
*end

*catchability[Qs]
potSurveyq: 1.15553e-05 
potCPUEq: 0.000240296 
*end

*partition_mean_length[growth_length_at_age]
time_step: 1
male {L}
mean_lengths {d}
year 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 
2019 23.8653 28.1157 31.5957 34.4448 36.7775 38.6874 40.251 41.5312 42.5794 43.4375 44.1401 44.7154 45.1863 45.5719 45.8876 46.1461 46.3577 46.531 
end {L}
female {L}
mean_lengths {d}
year 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 
2019 20.3404 23.898 26.8401 29.273 31.2849 32.9487 34.3246 35.4624 36.4034 37.1815 37.8249 38.357 38.7971 39.161 39.4619 39.7107 39.9165 40.0867 
end {L}
*end

*partition_mean_weight[growth_weight_at_age]
time_step: 1
male {L}
mean_weights {d}
year 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 
2019 0.00196958 0.00332558 0.00482671 0.00635615 0.00783036 0.00919687 0.0104278 0.0115131 0.0124542 0.0132598 0.0139422 0.0145153 0.0149934 0.0153898 0.0157168 0.0159852 0.0162046 0.0163831 
end {L}
female {L}
mean_weights {d}
year 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 
2019 0.00118001 0.00197826 0.00287018 0.00379057 0.00469074 0.00553829 0.00631434 0.00701014 0.00762399 0.00815878 0.00862007 0.00901485 0.00935058 0.00963463 0.00987397 0.010075 0.0102433 0.0103839 
end {L}
*end

*selectivity[potSSelm]
a50: 3.04015 
alpha: 1 
ato95: 0.19197 
intervals: 5 
label: potSurveySel_male 
length_based: 0 
partition_type: model 
type: logistic 
Values {v}
3 0.350731
4 1
5 1
6 1
7 1
8 1
9 1
10 1
11 1
12 1
13 1
14 1
15 1
16 1
17 1
18 1
19 1
20 1
*end

*selectivity[potSSelf]
a50: 3.3151 
alpha: 1 
ato95: 1.02651 
intervals: 5 
label: potSurveySel_female 
length_based: 0 
partition_type: model 
type: logistic 
Values {v}
3 0.288265
4 0.877028
5 0.9921
6 0.999548
7 0.999974
8 0.999999
9 1
10 1
11 1
12 1
13 1
14 1
15 1
16 1
17 1
18 1
19 1
20 1
*end

*observation[pot_survey]
observation_type: abundance
likelihood: lognormal
Values {d}
year category age length observed expected residual error_value process_error adjusted_error neglogLike pearsons_residuals
2010 male+female 0 0 11.066 14.0577 -2.99173 0.125 0.2 0.23585 -1.04213 -0.902345
2014 male+female 0 0 17.568 14.5545 3.01352 0.135 0.2 0.241299 -1.02191 0.858068
2018 male+female 0 0 12.532 11.9299 0.602146 0.215 0.2 0.293641 -1.1966 0.17189
*end

*observation[CPUE]
observation_type: biomass
likelihood: lognormal
Values {d}
year category age length observed expected residual error_value process_error adjusted_error neglogLike pearsons_residuals
1990 male+female 0 0 1.01179 1.27478 -0.262997 0.05 0.1 0.111803 -0.159256 -1.84527
1991 male+female 0 0 0.812523 1.08848 -0.275959 0.05 0.1 0.111803 1.10236 -2.26761
1992 male+female 0 0 0.789768 0.940395 -0.150627 0.05 0.1 0.111803 -1.05339 -1.43264
1993 male+female 0 0 0.800867 0.865337 -0.0644699 0.05 0.1 0.111803 -1.99001 -0.666372
1994 male+female 0 0 0.814346 0.849095 -0.0347487 0.05 0.1 0.111803 -2.14319 -0.366039
1995 male+female 0 0 0.844336 0.894917 -0.0505806 0.05 0.1 0.111803 -2.08542 -0.505529
1996 male+female 0 0 0.968863 0.961299 0.00756336 0.05 0.1 0.111803 -2.18618 0.0703722
1997 male+female 0 0 1.07802 1.00498 0.0730376 0.05 0.1 0.111803 -1.95939 0.650029
1998 male+female 0 0 1.0632 1.03787 0.0253337 0.05 0.1 0.111803 -2.1571 0.218324
1999 male+female 0 0 0.955824 1.07584 -0.120016 0.05 0.1 0.111803 -1.68858 -0.997786
2000 male+female 0 0 1.12167 1.1376 -0.0159242 0.05 0.1 0.111803 -2.19162 -0.125203
2001 male+female 0 0 1.23278 1.20709 0.0256925 0.05 0.1 0.111803 -2.16418 0.190376
2002 male+female 0 0 1.30543 1.23529 0.0701345 0.05 0.1 0.111803 -2.04222 0.507817
2003 male+female 0 0 1.26698 1.24571 0.0212627 0.05 0.1 0.111803 -2.17258 0.152667
2004 male+female 0 0 1.22875 1.24538 -0.0166265 0.05 0.1 0.111803 -2.19202 -0.119411
2005 male+female 0 0 1.31587 1.21757 0.0983004 0.05 0.1 0.111803 -1.91112 0.722115
2006 male+female 0 0 1.26358 1.15126 0.112324 0.05 0.1 0.111803 -1.79719 0.872658
2007 male+female 0 0 1.08656 1.07104 0.0155211 0.05 0.1 0.111803 -2.17704 0.129618
2008 male+female 0 0 1.01899 1.01349 0.00550843 0.05 0.1 0.111803 -2.18868 0.0486133
2009 male+female 0 0 1.03144 0.977829 0.0536111 0.05 0.1 0.111803 -2.05121 0.490385
2010 male+female 0 0 0.895963 0.93986 -0.0438966 0.05 0.1 0.111803 -2.1244 -0.417747
2011 male+female 0 0 0.984095 0.904648 0.0794465 0.05 0.1 0.111803 -1.86529 0.785489
2012 male+female 0 0 0.975651 0.884443 0.091208 0.05 0.1 0.111803 -1.75578 0.922376
2013 male+female 0 0 0.960914 0.872513 0.0884007 0.05 0.1 0.111803 -1.76945 0.90621
2014 male+female 0 0 1.00118 0.850126 0.151052 0.05 0.1 0.111803 -1.03421 1.58923
2015 male+female 0 0 0.928886 0.852284 0.076602 0.05 0.1 0.111803 -1.85139 0.803897
2016 male+female 0 0 0.917794 0.878162 0.0396324 0.05 0.1 0.111803 -2.09207 0.403664
2017 male+female 0 0 0.922603 0.885675 0.036928 0.05 0.1 0.111803 -2.10498 0.372929
2018 male+female 0 0 0.758655 0.882597 -0.123942 0.05 0.1 0.111803 -1.34659 -1.25603
*end

*observation[LFlogbook]
observation_type: proportions_at_length
likelihood: multinomial
Values {d}
year category age length observed expected residual error_value process_error adjusted_error neglogLike pearsons_residuals
2010 male+female 0 13 0 2.57679e-08 -2.57679e-08 100 0 100 0 -0.00160524
2010 male+female 0 14 0 1.4981e-07 -1.4981e-07 100 0 100 0 -0.00387052
2010 male+female 0 15 0 7.58679e-07 -7.58679e-07 100 0 100 0 -0.00871022
2010 male+female 0 16 0 3.36295e-06 -3.36295e-06 100 0 100 0 -0.0183384
2010 male+female 0 17 0 1.31197e-05 -1.31197e-05 100 0 100 0 -0.0362214
2010 male+female 0 18 0 4.53034e-05 -4.53034e-05 100 0 100 0 -0.0673093
2010 male+female 0 19 0 0.000139199 -0.000139199 100 0 100 0 -0.117991
2010 male+female 0 20 0 0.000382287 -0.000382287 100 0 100 0 -0.195559
2010 male+female 0 21 0 0.000941739 -0.000941739 100 0 100 0 -0.307022
2010 male+female 0 22 0 0.00208684 -0.00208684 100 0 100 0 -0.457297
2010 male+female 0 23 0 0.00417118 -0.00417118 100 0 100 0 -0.647198
2010 male+female 0 24 0 0.00754682 -0.00754682 100 0 100 0 -0.872021
2010 male+female 0 25 0 0.0124223 -0.0124223 100 0 100 0 -1.12154
2010 male+female 0 26 0.000289101 0.0187367 -0.0184476 100 0 100 0.0989739 -1.36051
2010 male+female 0 27 0.00289101 0.0261423 -0.0232513 100 0 100 0.947279 -1.45723
2010 male+female 0 28 0.00462561 0.0341211 -0.0294955 100 0 100 1.44097 -1.62474
2010 male+female 0 29 0.0118531 0.0421532 -0.0303 100 0 100 3.84225 -1.50793
2010 male+female 0 30 0.0153223 0.0498114 -0.034489 100 0 100 4.90356 -1.5853
2010 male+female 0 31 0.022839 0.0567339 -0.0338949 100 0 100 7.52386 -1.4652
2010 male+female 0 32 0.0297774 0.0625375 -0.0327602 100 0 100 10.0181 -1.353
2010 male+female 0 33 0.0760335 0.0667834 0.0092501 100 0 100 30.3419 0.370528
2010 male+female 0 34 0.082972 0.0690467 0.0139253 100 0 100 33.4241 0.549249
2010 male+female 0 35 0.100607 0.069057 0.0315502 100 0 100 42.1379 1.24433
2010 male+female 0 36 0.113328 0.0668268 0.0465007 100 0 100 48.9824 1.8621
2010 male+female 0 37 0.0913559 0.0626914 0.0286645 100 0 100 38.4094 1.1825
2010 male+female 0 38 0.10292 0.0572346 0.0456854 100 0 100 45.2364 1.96674
2010 male+female 0 39 0.0777681 0.0511311 0.026637 100 0 100 33.2531 1.20932
2010 male+female 0 40 0.0526164 0.0449742 0.00764218 100 0 100 21.5598 0.368747
2010 male+female 0 41 0.0436542 0.039153 0.00450122 100 0 100 17.888 0.232071
2010 male+female 0 42 0.0416305 0.0338174 0.00781313 100 0 100 17.5259 0.43224
2010 male+female 0 43 0.0222608 0.0289247 -0.00666391 100 0 100 8.7987 -0.39762
2010 male+female 0 44 0.0268864 0.0243395 0.00254691 100 0 100 11.4049 0.165275
2010 male+female 0 45 0.0141659 0.0199451 -0.00577913 100 0 100 5.77341 -0.413351
2010 male+female 0 46 0.0121422 0.015725 -0.00358273 100 0 100 5.14682 -0.287979
2010 male+female 0 47 0.0153223 0.0117836 0.00353872 100 0 100 7.11233 0.32793
2010 male+female 0 48 0.0104076 0.00830077 0.00210686 100 0 100 5.00449 0.232213
2010 male+female 0 49 0.00636022 0.00544642 0.000913799 100 0 100 3.20795 0.12416
2010 male+female 0 50 0.00751662 0.00330456 0.00421206 100 0 100 4.20985 0.733934
2010 male+female 0 51 0.00375831 0.0018441 0.00191421 100 0 100 2.24832 0.446168
2010 male+female 0 52 0.00520382 0.000942938 0.00426088 100 0 100 3.5054 1.38823
2010 male+female 0 53 0.00260191 0.000440707 0.0021612 100 0 100 1.91 1.02971
2010 male+female 0 54 0.0011564 0.000188019 0.000968385 100 0 100 0.935749 0.706298
2010 male+female 0 55 0.000289101 7.3185e-05 0.000215916 100 0 100 0.259287 0.2524
2010 male+female 0 56 0.000867303 2.59943e-05 0.000841308 100 0 100 0.871544 1.65014
2010 male+female 0 57 0.000289101 8.43006e-06 0.000280671 100 0 100 0.321768 0.966682
2010 male+female 0 58 0.000289101 2.49843e-06 0.000286602 100 0 100 0.356926 1.8132
*end

*estimate_summary[estimated_summary]
process[Recruitment].b0 {L}
value: 14210.7
std_dev: 1
estimation_phase: 
label: B0 
lower_bound: 100 
mcmc: 
parameter: process[Recruitment].b0 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: uniform_log 
upper_bound: 250000 
end {L}

age_length[asMm0].linf {L}
value: 47.3135
std_dev: 1
estimation_phase: 
label: male_Linf 
lower_bound: 30 
mcmc: 
parameter: age_length[asMm0].linf 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: uniform 
upper_bound: 70 
end {L}

age_length[asMm0].cv_last {L}
value: 0.0614247
std_dev: 1
estimation_phase: 
label: male_cv2 
lower_bound: 0.02 
mcmc: 
parameter: age_length[asMm0].cv_last 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: uniform 
upper_bound: 1.0 
end {L}

selectivity[potSurveySel_male].a50 {L}
value: 3.04015
std_dev: 1
estimation_phase: 
label: potSurvey_mA50 
lower_bound: 1 
mcmc: 
parameter: selectivity[potSurveySel_male].a50 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: uniform 
upper_bound: 7 
end {L}

selectivity[potSurveySel_male].ato95 {L}
value: 0.19197
std_dev: 1
estimation_phase: 
label: potSurvey_mAto95 
lower_bound: .1 
mcmc: 
parameter: selectivity[potSurveySel_male].ato95 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: uniform 
upper_bound: 5 
end {L}

selectivity[potSurveySel_female].a50 {L}
value: 3.3151
std_dev: 1
estimation_phase: 
label: potSurvey_fA50 
lower_bound: 1 
mcmc: 
parameter: selectivity[potSurveySel_female].a50 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: uniform 
upper_bound: 7 
end {L}

selectivity[potSurveySel_female].ato95 {L}
value: 1.02651
std_dev: 1
estimation_phase: 
label: potSurvey_fAto95 
lower_bound: .1 
mcmc: 
parameter: selectivity[potSurveySel_female].ato95 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: uniform 
upper_bound: 5 
end {L}

process[Recruitment].ycs_values{1980} {L}
value: 0.23512
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1980} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{1981} {L}
value: 0.232089
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1981} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{1982} {L}
value: 0.233169
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1982} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{1983} {L}
value: 0.241993
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1983} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{1984} {L}
value: 0.264699
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1984} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{1985} {L}
value: 0.314953
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1985} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{1986} {L}
value: 0.416347
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1986} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{1987} {L}
value: 0.601999
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1987} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{1988} {L}
value: 0.90255
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1988} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{1989} {L}
value: 1.37986
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1989} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{1990} {L}
value: 1.55858
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1990} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{1991} {L}
value: 1.02851
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1991} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{1992} {L}
value: 0.8478
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1992} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{1993} {L}
value: 0.847081
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1993} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{1994} {L}
value: 1.06575
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1994} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{1995} {L}
value: 1.5481
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1995} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{1996} {L}
value: 1.05337
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1996} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{1997} {L}
value: 0.802935
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1997} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{1998} {L}
value: 0.816576
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1998} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{1999} {L}
value: 0.628936
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{1999} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{2000} {L}
value: 0.802337
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{2000} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{2001} {L}
value: 0.745992
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{2001} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{2002} {L}
value: 0.466754
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{2002} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{2003} {L}
value: 1.09063
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{2003} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{2004} {L}
value: 0.917104
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{2004} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{2005} {L}
value: 0.38256
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{2005} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{2006} {L}
value: 0.901903
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{2006} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{2007} {L}
value: 0.720451
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{2007} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{2008} {L}
value: 0.834791
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{2008} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{2009} {L}
value: 0.426064
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{2009} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{2010} {L}
value: 1.28707
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{2010} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{2011} {L}
value: 0.691099
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{2011} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{2012} {L}
value: 0.736775
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{2012} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{2013} {L}
value: 0.561903
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{2013} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

process[Recruitment].ycs_values{2014} {L}
value: 0.439778
std_dev: 1
cv: 0.6 
estimation_phase: 
label: YCS 
lower_bound: 0.1 
mcmc: 
mu: 1 
parameter: process[Recruitment].ycs_values{2014} 
prior_applies_to_transform: 
same: 
transform_with_jacobian: 
transformation: 
type: lognormal 
upper_bound: 10 
end {L}

*end

*hessian_matrix[Hess]
hessian_matrix {m}
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
*end

*covariance_matrix[Covar]
covariance_matrix {m}
1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 
*end

*correlation_matrix[Corr]
correlation_matrix {m}
1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 
*end

*warnings[warnings_encounted]
warnings_found: 1
warning_0 {s}
Estimates were removed because of matching lower and upper bounds. Originally had 127 estimates, now have 42
*end

Total elapsed time: 10.5 minutes
Completed

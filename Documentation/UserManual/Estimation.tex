\section{\I{The estimation section: estimation methods and parameters}\label{sec:Estimation}}

The command and subcommand syntax for the estimation section is given in Section \ref{syntax:Estimation}.

\subsection{\I{Role of the estimation section}\label{sec:role-of-the-estimation-section}}

The role of the estimation section is to define the tasks carried out by \CNAME:

\begin{enumerate}
  \item Define the objective function (see Section \ref{sec:objective-function})
  \item Define the parameters to be estimated (the free parameters, see Section \ref{sec:FreeParameters})
  \item Calculate a point estimate, i.e., the maximum posterior density estimate (MPD) (see Section \ref{sec:estimate-MPD})\index{Maximum posterior density estimate (MPD)}\index{MPD (Maximum posterior density estimate)}
  \item Calculate a posterior profile on selected parameters, i.e., for each of a series of values of a parameter, minimise the objective function, allowing the other estimated parameters to vary (see Section \ref{sec:Profile}\index{Profiles})
  \item Generate MCMC\index{Markov chain Monte Carlo (MCMC)} samples from the posterior distribution (see Section \ref{sec:MCMC}\index{MCMC})
  \item Calculate the approximate covariance matrix of the parameters as the inverse of the minimizer\textquoteright{}s approximation to the \I{Hessian}, and the corresponding correlation matrix (see Section \ref{sec:estimate-MPD}\index{Covariance matrix})
\end{enumerate}

The estimation section defines the objective function, parameters of the model, and the method of estimation (point estimates, Bayesian posteriors, profiles, etc.). The objective function is based on a goodness-of-fit measure of the model to observations, the assumed priors, and the penalties. See the observation section for a description of the observations, likelihoods, priors, and penalties.

\subsection{\I{The objective function}\label{sec:objective-function}}

In Bayesian estimation, the objective function is a negative log-posterior,

\begin{equation}\label{objective_function}
Objective(p)= - \sum\limits_i {\log \left[ {L\left( {{\bf{p}}|O_i } \right)} \right]}  - \log \left[ {\pi \left( {\bf{p}} \right)} \right]
\end{equation}

where $\pi$ is the joint prior density of the parameters $p$.

The contribution to the objective function from the likelihood components is described in Section \ref{sec:Likelihood}. In addition to likelihoods, priors (see Section \ref{sec:Priors}) and penalties (see Section \ref{sec:Penalty}) are components of the objective function. Note that if the priors are specified as uniform, then the prior contribution is zero and the optimisation is now a penalised likelihood and not Bayesian.

Penalties can be used to ensure that the estimated parameter values and derived quantities meet certain restrictions. For example, exploitation rate constraints on mortality events (i.e., fisheries) that are not violated (otherwise there is nothing to prevent the model from having abundances so low that the recorded catches could not have been taken); penalties on category transitions (to ensure there are enough individuals to move); penalties such that estimated values are similar or smooth, etc.

Equation~\ref{objective_function} can be reduced to a penalised likelihood equation if all priors are assumed to be uniform. This is because uniform priors have no contribution to the objective function so Equation~\ref{objective_function} reduces to the likelihood components plus penalties.

\subsection{\I{Specifying the parameters to be estimated}\label{sec:FreeParameters}}

The parameters to be estimated (estimables) are defined using \command{estimate} commands (see Section \ref{syntax:Estimation}).

For example, a \command{estimate} command block \index{Estimating parameters}

{\small{\begin{verbatim}
  @estimate male.m
  parameter process[NaturalMortality].m{male}
  lower_bound 0.1
  upper_bound 0.4
  type uniform
\end{verbatim}}}

See Section \ref{sec:parameter-names} for information on how to specify the parameter name. At least one parameter is required to be estimated if doing an estimation \texttt{-e}, profile \texttt{-p}, or MCMC \texttt{-m} run. Initial values for the parameters to be estimated are required, and these values are used as the starting values for the minimiser. However, these values may be overwritten if a set of alternative starting values is provided (i.e., using \texttt{\cname\ -i}, see Section \ref{sec:CommandLineArguments}).

All parameters are estimated within the specified bounds\index{Bounds}. For each parameter estimated, the lower and upper bounds and the prior\index{Priors} (\texttt{type}) (Section \ref{sec:Priors}) must be specified. The bounds and the prior should be chosen carefully as they affect the values over which the minimisers search. Some minimisers convert the lower and upper bounds into a minimisation space (for example -1,1 space for the numerical differences algorithm). If estimating only some elements of a vector, either define each element of the vector to be estimated or fix the others by setting the the lower and upper bounds to the same value as the initial value.

\subsection{\I{Point estimation}\label{sec:estimate-MPD}}\label{sec:Minimiser}

Point estimation is invoked with \texttt{\cname\ -e}, which attempts to find a minimum of the objective function\index{Objective function}. \CNAME\ has multiple minimisation algorithms. There are two automatic differentiation (AD) minimisers: ADOL-C, and BetaDiff (the minimiser used in CASAL). There are also three non-automatic differentiation minimisers: numerical differences, DeltaDiff, and the differential evolution minimiser (\subcommand{de\_solver}). Automatic differentiation  minimisers are recommended for more complex models as they are on average much faster and tend to find a more robust minimum when exploring a complex objective surface.

An important input parameter for most minimisers is the \subcommand{tolerance} parameter. This is the gradient of the objective function, and is used as the stopping rule to define the 'solution' (although a solution may be a local minimum and not the global minimum). Evaluating the robustness of a minimum can be tested with different starting values (i.e., using \texttt{-i free\_parameter\_file.txt}).

Start with the default \subcommand{tolerance} parameter value of 1e-5 and decrease it while developing a model. For a given model, the parameter estimates when minimising with different tolerance may be different.

\subsubsection{\I{The numerical differences minimiser}}\label{sec:Minimiser-GammaDiff}

See Section \ref{syntax:Minimiser-GammaDiff} for the command syntax.

The numerical differences minimiser uses a quasi-Newton minimiser which is a slightly modified implementation of the main algorithm of \cite{779}, and uses an $arcsin$ transformation to ensure parameters remain within bounds.

The minimiser has three kinds of (non-error) exit status, depending on the minimiser:

\begin{itemize}
\item Successful convergence (suggests a local minimum has been found, at least).
\item Convergence failure (a local minimum has not been found, although the results may be 'close enough').
\item Convergence unclear (the minimiser halted but was unable to determine if convergence occurred. The result may be a local minimum, although this can be checked by restarting the minimiser at the final values of the estimated parameters).
\end{itemize}

The maximum number of quasi-Newton iterations\index{Quasi-Newton iterations} and objective function evaluations\index{Objective function evaluations} allowed can be specified. If either limit is exceeded, the minimiser exits with a convergence failure\index{Convergence failure}. Set the maximum number of evaluations and iterations to values larger than the defaults of 300 and 1000, unless convergence is reached with fewer. An alternative starting point of the minimiser can be specified using \texttt{\cname\ -i}.

The minimisers are local optimisation algorithms trying to solve a global optimisation problem. What this means is that, even if a 'successful convergence'\index{Successful convergence} is reached, the solution may be only a local minimum\index{Local minimums}, and not a global one. To diagnose this problem, start multiple runs from different starting points and comparing the results, or do profiles of one or more key parameters and seeing if any of the profiled estimates finds a better optimum than than the original point estimate.

The approximate covariance matrix\index{Covariance matrix} of the estimated parameters can be calculated as the inverse of the minimiser's approximation to the \I{Hessian}, and the corresponding correlation matrix\index{Correlation matrix} is also calculated.

Note that

\begin{itemize}
\item the Hessian approximation develops over many minimiser steps, so if the minimiser has only run for a small number of iterations the covariance matrix can be a very poor approximation; and
\item the inverse Hessian is not a good approximation to the covariance matrix of the estimated parameters, and may not be useful to construct, for example, confidence intervals.
\end{itemize}

Also note that if an estimated parameter has equal lower and upper bounds, it will have entries of '0' in the covariance matrix and \texttt{NaN} or \texttt{-1.\#IND} (depending on the operating system) in the correlation matrix.

{\small{\begin{verbatim}
@minimiser numerical_diff
type numerical_differences
tolerance 1e-6
iterations 2500
evaluations 4000
\end{verbatim}}}

\subsubsection{\I{The DeltaDiff numerical differences minimiser}}\label{sec:Minimiser-DeltaDiff}

See Section \ref{syntax:Minimiser-DeltaDiff} for the command syntax.

DeltaDiff applies the same minimiser as Numerical Differences, expect that it uses $tan$ rescaling for the parameters rather than $arcsin$. This minimiser may perform better than the Numerical Differences minimiser when parameters are very close to zero bounds.

\subsubsection{\I{The differential evolution minimiser}}\label{sec:Minimiser-DESolver}

The differential evolution minimiser is a simple population-based, stochastic function minimizer, but is claimed to be quite powerful in solving minimisation problems. It is a method of mathematical optimization of multidimensional functions and belongs to the class of evolution strategy optimizers.

Initially, the procedure randomly generates and evaluates a number of solution vectors (the population size), each with $p$ parameters. Then, for each generation (iteration), the algorithm creates a candidate solution for each existing solution by random mutation and uniform crossover. The random mutation generates a new solution by multiplying the difference between two randomly selected solution vectors by some scale factor, then adding the result to a third vector. Then an element-wise crossover takes place with probability $P_{cr}$, to generate a potential candidate solution. If this is better than the initial solution vector, it replaces it, otherwise the original solution is retained. The algorithm terminates after either a predefined number of generations (\argument{max\_generations}) or when the maximum difference between the scaled individual parameters from the candidate solutions from all populations is less than some predefined amount \argument{tolerance}.

The differential evolution minimiser can be good at finding global minimums in surfaces that may have local minima. However, the speed of the minimiser, and the ability to find a good minima depend on the number of initial 'populations'. Some authors recommend that the number of populations be set at about $10*p$, where $p$ is the number of free parameters. However, depending on the model, this value can be set to a lower value and still find a robust solution.

There is no proof of convergence for the differential evolution solver, but several papers have found it to be an efficient method of solving multidimensional problems. Some results suggest that it can often find a better minima and may be faster or longer (depending on the actual model specification) at finding a solution when compared with the numerical differences minimiser. Comparisons with automatic differentiation minimisers or other more sophisticated algorithms have not been made.

{\small{\begin{verbatim}
		@minimiser DE_solver
		type de_solver
		tolerance 1e-6
		iterations 2500
		evaluations 4000
\end{verbatim}}}

\subsubsection{\I{The BetaDiff minimiser}}\label{sec:Minimiser-BetaDiff}

An automatic differentiation minimiser for non-linear models, This is the minimiser from the original CASAL package, based on ADOL-C.

{\small{\begin{verbatim}
		@minimiser beta_diff
		type beta_diff
		tolerance 1e-6
		iterations 2500
		evaluations 4000
\end{verbatim}}}

\subsubsection{\I{The ADOL-C minimiser}}\label{sec:Minimiser-ADOLC}

An automatic differentiation minimiser for non-linear models. See \url{https://projects.coin-or.org/ADOL-C} for more information. Users do have an option of defining what transformation to apply to convert the parameter \(\theta \in [\theta_{LB}, \theta_{UB}]\) to \(X \in [-1, 1]\), for which optimisation is done. The options are sin or tan. Initial model runs suggest this assumption will make a difference to convergence, particularly if there are poorly identified parameters which fall at the bounds, we have found the sin transform is more consistent with the betadiff minimser. The sin transform
\begin{equation}
	X = \frac{asin(2 * (\theta - \theta_{LB}) / (\theta_{UB} - \theta_{LB}) - 1)}{ 1.57079633}
\end{equation}
%
the real consequence of this transformation is when \(X\) is back transformed to \(\theta\) there is a penalty which is added to the minimisation to dissuade parameter values close to the bounds. This penalty is hidden from the reported objective function. If you are interested in it, you can run with \texttt{--loglevel medium} and it should be reported. The back transformation follows,
\begin{equation}
\theta = \theta_{LB} + (\theta_{UB} - \theta_{LB}) * (sin(X * 1.57079633) + 1) / 2;
\end{equation}
%
and penalty
\begin{align}
	&if(-0.9999 - X < 0) & penalty += (X + 0.9999)^2\\
	&if(X - 0.9999 < 0) & penalty += (X - 0.9999)^2
\end{align}
%
This can be seen here \href{https://github.com/NIWAFisheriesModelling/CASAL2/blob/1b1ed731537dc551674c911da3bf387273a97a92/CASAL2/source/Utilities/Math.h#L245}{here}.


The Tan transform uses transformation

\begin{equation}
X = tan(((\theta - \theta_{LB}) / (\theta_{UB} - \theta_{LB}) - 0.5) * \pi)
\end{equation}
%
and back transform
\begin{equation}
\theta = ((atan(X) / \pi) + 0.5) * (\theta_{UB} - \theta_{LB}) + \theta_{LB}
\end{equation}

{\small{\begin{verbatim}
		@minimiser ADOLC
		type adolc
		step_size 1e-6
		iterations 2500
		evaluations 4000
		tolerance 1e-6
		parameter_transformation sin_transform
\end{verbatim}}}

\subsection{\I{Posterior profiles}}\label{sec:Profile}

If profiles are run using the command \texttt{\cname\ -p}, \CNAME\ will first calculate a point estimate. For each scalar parameter or, in the case of vectors or selectivities, the element of the parameter to be profiled, \CNAME\ will fix its value at a sequence of $n$ evenly spaced numbers ($step$) between the specified lower and upper bounds $l$ and $u$, and calculate a point estimate at each value.

By default $step=10$, and $(l, u)=($lower bound on parameter plus $(range/(2n))$, upper bound on parameter less $(range/(2n))$. Each minimisation starts at the final parameter values from the previous resulting value of the parameter being profiled. \CNAME\ will report the objective function for each parameter value. The initial point estimate should be compared with the profile results, to check at least that none of the other points along the profile have a better objective function value than the initial 'minimum'.

The parameters to be profiled are specified, and optionally the number of steps, and lower bound and upper bound, for each parameter. In the case of vector parameters, the element(s) of the vector to be profiled are specified.

The initial starting point for the estimation can also be specified using \texttt{\cname\ -i \emph{file}}, which may improve the minimiser performance for the profiles.

If the profile results are not reasonable, it may be a result of not using enough iterations in the minimiser or a poor choice of minimiser control variables (e.g., the minimiser tolerance). It may also be useful to try other minimisers and compare the results.

\subsection{\I{Bayesian estimation}}\label{sec:MCMC}\label{sec:MCMC-RandomWalkMetropolisHastings}

\CNAME\ can use \I{Markov chain Monte Carlo (MCMC)}\index{MCMC} functionality to generate a sample from the posterior distribution of the estimated parameters with command \texttt{\cname\ -m} and output the sampled values to a file, optionally keeping only every $n$th set of values.

As \CNAME\ has no post-processing capabilities. \CNAME\ cannot produce MCMC convergence diagnostics. To calculate these diagnostics, use a package such as \href{http://www.public-health.uiowa.edu/boa}{BOA}, plot/summarize the posterior distributions of the output quantities, and/or use a general-purpose statistical package such as \href{http://www.r-project.org}{\R}.

Bayesian methodology\index{Bayesian estimation} and MCMC are both large and complex topics. See Gelman et al. \citeyearpar{823} and Gilks et al. \citeyearpar{143} for details of both Bayesian analysis and MCMC methods. In addition, see Punt \& Hilborn \citeyearpar{828} for an introduction to quantitative fish stock assessment using Bayesian methods.

This section briefly describes the MCMC algorithms used in \CNAME. See Section \ref{syntax:MCMC} for the \CNAME\ commands used in an MCMC Bayesian analysis.

\CNAME\ implements two methods for MCMC. The first is a straightforward implementation of the random walk Metropolis-Hastings algorithm \citep{823,143}. The Metropolis-Hastings algorithm attempts to draw a sample from a Bayesian posterior distribution, and calculates the posterior density $\pi$, scaled by an unknown constant. The algorithm generates a 'chain' or sequence of values. Typically the beginning of the chain is discarded (the burn-in period) and every $N$th element of the remainder is taken as the posterior sample. The second is Hamiltonian Monte Carlo. This uses similar subcommands as the random walk Metropolis-Hastings algorithm. In both cases, the chain is produced by taking an initial point $x_0$ and repeatedly applying the following rule, where $x_i$ is the current point:

\begin{enumerate}
\item Draw a candidate step s from a proposal distribution J, which should be symmetric i.e., $J(-s)=J(s)$
\item Calculate $r=min(\pi(x_i+s)/\pi(x_i),1)$
\item Let $x_{i+1}=x_i+s$ with probability $r$, or $x_i$ with probability $1-r$
\end{enumerate}

An initial point estimate is produced before the chain starts, which is done so as to calculate the approximate covariance matrix of the estimated parameters (as the inverse Hessian), and may also be used as the starting point of the chain.

The starting point of the point estimate minimiser can be specified using the command \texttt{\cname\ -i}. Don't start it too close to the actual estimate (either by using \texttt{\cname\ -i}, or by changing the initial parameter values in \config) as it takes a few iterations to determine a reasonable approximation to the Hessian.

There are currently two options for the starting point of the MCMC:

\begin{itemize}
\item Start from the point estimate; or
%\item Start from a random point near the point estimate (the point is generated from a multivariate normal distribution, centred on the point estimate, with covariance equal to the inverse Hessian multiplied by a user-specified constant). This may be useful if the chain gets `stuck' at the point estimate, or if you wish to generate multiple chains from  for later MCMC diagnostic tests.
\item Restart a chain given a covariance matrix and a previous starting point.
\end{itemize}

The chain moves in natural space, i.e., no transformations are applied to the estimated parameters. The default proposal distribution is a multivariate Student's $t$ distribution centred on the current point, with covariance matrix equal to a matrix based on the approximate covariance produced by the minimiser, multiplied by a step size factor.

The following steps define how the initial covariance matrix of the proposal distribution is calculated:

\begin{enumerate}
\item The covariance matrix is taken as the inverse of the approximate Hessian from the quasi-Newton minimiser.

\item The covariance matrix is modified so as to decrease all correlations greater than \commandsub{mcmc}{max\_correlation} down to \commandsub{mcmc}{max\_correlation}, and similarly to increase all correlations less than -\commandsub{mcmc}{max\_correlation} up to -\commandsub{mcmc}{max\_correlation} (the \commandsub{mcmc}{max\_correlation} parameter defaults to 0.8). This should help to avoid getting 'stuck' in a lower-dimensional subspace.

\item The covariance matrix is then modified either by

\begin{itemize}
\item  \commandsubarg{mcmc}{adjustment\_method}{covariance}: that if the variance of the $i$th parameter is non-zero and less than \commandsub{mcmc}{min\_difference} multiplied by the difference between the parameters' lower and upper bound, then the variance is changed, without changing the associated correlations, to $k=$min\_diff$(upper\_bound_i-lower\_bound_i)$. This is done by setting \[
{\mathop{\rm Cov}\nolimits} \left( {i,j} \right)^\prime   = {{{\mathop{\rm sqrt}\nolimits} \left( k \right){\mathop{\rm Cov}\nolimits} \left( {i,j} \right)} \mathord{\left/
{\vphantom {{{\mathop{\rm sqrt}\nolimits} \left( k \right){\mathop{\rm Cov}\nolimits} \left( {i,j} \right)} {{\mathop{\rm sd}\nolimits} \left( i \right)}}} \right.
\kern-\nulldelimiterspace} {{\mathop{\rm sd}\nolimits} \left( i \right)}}
\]
for $i \ne j$, and ${\mathop{\rm var}} \left( i \right)^\prime   = k$

\item \commandsubarg{mcmc}{adjustment\_method}{correlation}: that if the variance of the $i$th parameter is non-zero and less than \commandsub{mcmc}{min\_difference} multiplied by the difference between the parameters' lower and upper bounds, then its variance is changed to $k=min\_diff(upper\_bound_i-lower\_bound_i)$. This differs from (i) above in that the effect of this option is that it also modifies the resulting correlations between the $i$th parameter and all other parameters.
\end{itemize}

This allows each estimated parameter to move in the MCMC even if its variance is very small according to the inverse Hessian. In both cases, the \commandsub{mcmc}{min\_difference} parameter defaults to $0.0001$.

\item The \commandsub{mcmc}{stepsize} (a scalar factor applied to the covariance matrix to improve the acceptance probability) is set by the user. The default is $2.4d^{-0.5}$ where $d$ is the number of estimated parameters, as recommended by Gelman et al. \citep{823}.
\end{enumerate}

The proposal distribution can also change adaptively during the chain, using two different mechanisms. Both are offered as means of improving the convergence properties of the chain. It is important to note that any adaptive behaviour must finish before the end of the burn-in period, i.e., the proposal distribution must be finalised before the kept portion of the chain starts.

The adaptive mechanisms are:

\begin{itemize}
\item The step size changes adaptively at one or more sample numbers (See next paragraph for details on the step size adaptation methods)
\item The entire covariance matrix changes adaptively at one or more sample numbers. At each adaptation, the covariance matrix is replaced with an empirical covariance matrix, derived from the MCMC chain. The idea is that an empirical covariance is a better approximation of the proposal distribution than the inverse of the Hessian matrix, and can improve convergence and mixing of the chain.
\end{itemize}

The two options to adapt the step size are \texttt{double\_half} or \texttt{ratio}, which is chosen with the input parameter \texttt{adapt\_stepsize\_method}. The \texttt{double\_half} method is used in CASAL (see \cite{823} for justification).

The algorithm for \texttt{double\_half} is, at each adaptation, the step size is doubled if the acceptance rate since the last adaptation is more than $0.5$, or halved if the acceptance rate is less than $0.2$. The \texttt{ratio} is taken from SPM. It adapts the current step size by the acceptance rate since the last adaptation multiplied by 4.1667 to approach an acceptance rate of $\approx$ 0.24. See \cite{mcmc_rate} for justification on that acceptance rate.

The \texttt{stepsize} parameter is now on a completely different scale, and must be rescaled. It is set to a user-specified value (which may or may not be the same as the initial step size). Set the step size adaptations to occur after this, so that the step size can be readjusted to an appropriate value which gives good acceptance probabilities with the new matrix.

All modified versions of the covariance matrix are printed to the standard output, but only the initial covariance matrix (inverse Hessian) is saved to the objectives file (see Section \ref{syntax:Report-MCMCCovariance}). \label{sec:Report-MCMCCovariance}  

The variance-covariance matrix of this sub-sample of chain is calculated. As above, correlations greater than \commandsub{mcmc}{max\_correlation} are reduced to \commandsub{mcmc}{max\_correlation}, correlations less than -\commandsub{mcmc}{max\_correlation} are increased to  -\commandsub{mcmc}{max\_correlation}, and very small non-zero variances are increased (\commandsub{mcmc}{covariance\_adjustment} and \commandsub{mcmc}{min\_difference}). The result is the new variance-covariance matrix of the proposal distribution. 

The procedure used to choose the sample of points is that, to start, all points on the chain so far are taken. \TODO{reword this paragraph} All points in an initial user-specified period are discarded. The assumption is that the chain will have started moving during this period. If this is incorrect and the chain has still not moved by the end of this period, it is a fatal error and \CNAME\ stops. The remaining set of points must contain at least some user-specified number of transitions. If this is incorrect and the chain has not had at least this number of transitions, then it is also a fatal error. If this test is passed, the set of points is systematically sub-sampled down to 1000 points (and it must be at least this long to start with).

The probability of acceptance for each jump is $0$ if the jump would move a parameter value outside of its bounds, $1$ if it improves the posterior, or $(new posterior/old posterior)$ otherwise. How often the position of the chain is recorded is specified with the \texttt{keep} parameter. For example, with \texttt{keep 10}, only every $10$th sample is recorded.

\TODO [this option is currently disabled]: The option to specify that some of the estimated parameters are fixed during the MCMC is available. If the chain starts at the point estimate or at a random location, these fixed parameters are set to their values at the point estimate.

If  the start of the chain is specified with the command \texttt{\cname\ -i}, these fixed parameters are set to the values in the file.

Restarting an MCMC chain: in the case where an MCMC chain was halted or interrupted, the MCMC chain can be restarted from where it finished with

{\small{\begin{verbatim}
casal2 -R MPD_file --objective-file objectives_file --sample-file samples_file
\end{verbatim}}}

where \texttt{Objective\_file\_name} is the file name for the objective function report and \texttt{Sample\_file\_name} is the file name for the sample report from a MCMC chain.

The posterior sample can be used for (projections (Section \ref{sec:Project})) or simulations (see Section \ref{sec:Simulate}) with the values supplied with the command \texttt{\cname\ -i \emph{file}}.

A multivariate Student's $t$ distribution is used as an alternative to the multivariate normal proposal distribution. If you request multivariate Student's $t$ proposals, change the degrees of freedom from the default of 4. As the degrees of freedom decreases, the $t$ distribution becomes more heavy tailed. This may lead to better convergence properties. Note the default is the multivariate Student's $t$.

Given a posterior (sub)sample, \CNAME\ can calculate a list of output quantities for each sample point (see Section~\ref{sec:Report} specifically tabular report). These quantities can be output to a file (with the command \texttt{\cname\ -r --tabular}) and read into an external software package where the posterior distributions can be plotted and/or summarised.

The posterior sample can also be used for projections (Section~\ref{sec:Project}). The advantage of this is that the parameter uncertainty, as expressed in the posterior distribution, can be included into the risk estimates.

\subsection{Priors\label{sec:Priors}}

In a Bayesian analysis, a prior is required for every parameter that is being estimated. There are no default priors.

When some of these priors are parameterised in terms of mean, c.v., and standard deviation, these refer to the parameters of the distribution before the bounds are applied. The moments of the prior after the bounds are applied may differ.

\CNAME\ has the following priors (expressed in terms of their contribution to the objective function):


\subsubsection{Uniform\index{Uniform prior}\index{Priors ! Uniform}}\label{sec:Prior-Uniform}

\begin{equation}
 - \log \left(\pi \left(p \right) \right) = 0
\end{equation}

\subsubsection{Uniform-log\index{Uniform-log prior}\index{Priors ! Uniform-log}} (i.e., $\log(p) \sim \text{uniform}$)\label{sec:Prior-UniformLog}

\begin{equation}
 - \log \left(\pi \left(p \right) \right) = \log \left( p \right)
\end{equation}

\subsubsection{Normal\index{Normal prior}\index{Priors ! Normal}}\label{sec:Prior-Normal}

The normal distribution with mean $\mu$ and standard deviation with c.v $c$

\begin{equation}
 - \log \left(\pi \left(p \right) \right) = 0.5\left(\frac{p - \mu}{c\mu} \right)^2
\end{equation}

\subsubsection{Normal with standard deviation}\label{sec:Prior-NormalByStdev}

The normal distribution with mean $\mu$ and standard deviation $\sigma$

\begin{equation}
 - \log \left(\pi \left(p \right) \right) = 0.5\left(\frac{p - \mu}
{\sigma }\right)^2
\end{equation}

\subsubsection{Lognormal\index{Lognormal prior}\index{Priors ! Lognormal}}\label{sec:Prior-Lognormal}

The lognormal distribution with mean $\mu$ and c.v. $c$

\begin{equation}
 - \log \left(\pi \left(p \right) \right) = \log \left( p \right) + 0.5 \left( \frac{\log \left( p / \mu \right)}{s} + \frac{s}{2} \right)^2
\end{equation}

where $s$ is the standard deviation of $\log(p)$ and $s= \sqrt{\log \left(1+c^2 \right)}$.

\subsubsection{Normal-log\index{normal-log prior}\index{Priors ! Normal-log}}\label{sec:Prior-NormalLog}

The normal-log distribution  with mean $\mu$ and c.v. $c$

Similar to the lognormal prior, but with the mean ($mu$) and standard deviation ($sigma$) specified in log space.

where $s$ is the standard deviation of $\log(p)$ and $s= \sqrt{\log \left(1+c^2 \right)}$.


\subsubsection{Beta\index{Beta prior}\index{Priors ! Beta}}\label{sec:Prior-Beta}

The Beta distribution with mean $\mu$ and standard deviation $\sigma$, and range parameters $A$ and $B$

\begin{equation}
 - \log \left(\pi \left( p \right) \right) = \left( 1 - m \right) \log \left( p - A \right) + \left( 1 - n \right)\log \left( B - p \right)
\end{equation}

where $\nu  = \frac{\mu  - A}{B - A}$, and $\tau = \frac{\left(\mu -A \right)\left(B - \mu \right)}{\sigma ^2} - 1$ and then $\mu=\tau \nu$ and $n=\tau(1-\nu)$. Note that the beta prior is undefined when $\tau \leq 0$.

Vectors of parameters can be independently (but not necessarily identically) distributed according to any of the above forms, in which case the joint negative-log-prior for the vector is the sum of the negative-log-priors of the components. Values of each parameter need to be specified for each element of the vector. Example of syntax to define the estimation of a parameter and the prior assumed:

{\small{\begin{verbatim}
		## uniform-log example estimate
		@estimate B0
		type uniform_log	# this command "type" defines the prior type.
		parameter process[Recruitment].b0 # "Recruitment" is the label of your process
		upper_bound 20000
		lower_bound 1000

		## Lognormal YCS estimation
		@estimate year_class_strengths_1990_1995
		type lognormal
		parameter process[Recruitment].ycs_values{1990:1995}
		# ycs_year  1990	1991	1992	1993	1994	1995
		mu   		1   	1   	1   	1   	1   	1
		cv 			0.9 	0.9 	0.9 	0.9 	0.9 	0.9
		lower_bound 0.01	0.01	0.01	0.01	0.01	0.01
		upper_bound 9		9		9		9		9		9
\end{verbatim}}}

\subsection{Penalties}\label{sec:Penalty}\label{sec:Penalty-Process}

Penalties are associated with processes and can be used to enforce parameter value or derived quantity restrictions or model outputs that are invalid by adding a penalty to the objective function. For example, estimated parameter values can be restricted so that a known mortality event removes enough individuals from the population within an event mortality process. \CNAME\ requires penalty functions for processes that remove or shift a \emph{number} of individuals between categories or from the partition. Many of the penalties that were available in CASAL have been moved to be additional priors in \CNAME (see Section~\ref{sec:AdditionalPriors}).

For penalties, a multiplier is required to be specified, and the objective function is increased by this multiplier multiplied by the penalty value. In some cases the multiplier may need to be quite large to prohibit some model behaviour.

Penalties are implemented for the processes

\begin{itemize}
	\item \commandlabsubarg{process}{type}{event\_mortality},
	\item \commandlabsubarg{process}{type}{mortality\_instantaneous},
	\item \commandlabsubarg{process}{type}{tag\_by\_length},
	\item \commandlabsubarg{process}{type}{tag\_by\_length}, and
	\item \commandlabsubarg{process}{type}{category\_transition}
\end{itemize}

For these processes, two types of penalties can be defined: on the natural scale (the default) and on the log scale. Both of these types add a penalty value of the squared difference between the observed value (e.g., the actual number of individuals to be removed in an event mortality process or the actual number of individuals to shift in a category transition process), and the number that were moved (if less than or equal), multiplied by the penalty multiplier.

The natural scale penalty calculates the squared difference on a natural scale, and the log scale penalty calculates the squared difference of the logged values.

For example:

{\small{\begin{verbatim}
@process Mortality
type mortality_instantaneous
penalty CatchMustBeTaken

# define the penalty in an @penalty block
@penalty CatchMustBeTaken
type process
log_scale True
multiplier 10000
\end{verbatim}}}

Penalties are added to the objective function in the following ways;

\begin{equation}
	Penalty = (X_1 - X_2)^2
\end{equation}

or if \subcommand{log\_scale true}

\begin{equation}
Penalty = (log(X_1) - log(X_2))^2
\end{equation}

where, for example, $X_1$ is observed catch biomass and $X_2$ is the estimated catch biomass. Penalties are usually applied in situations when numbers or weight are known. Another example is for tagging, where the number of individuals that were tagged in a given year is known, so a penalty can be used to restrict the model to estimate reasonable values for the numbers of tagged individuals in that year.

\subsection{Additional Priors\label{sec:AdditionalPriors}}

Additional priors can be thought of as the inverse of penalties \TODO{please rephrase}. For CASAL models, most of the legacy \command{penalty} blocks have now been implemented as \command{additional\_prior} blocks. They restrict parameters in user-defined spaces \TODO{please rephrase}.

The types of additional priors available in \CNAME\ are \texttt{vector\_smoothing}, \texttt{vector\_averaging}, \texttt{uniform\_log}, \texttt{lognormal}, \texttt{element\_difference}, and \texttt{Beta}:

\begin{itemize}
	\item \texttt{vector\_average}\label{sec:AdditionalPrior-VectorAverage}

	This prior can be applied to a vector parameter. Sum of squares of $r^{th}$ differences, optionally on a log scale. This encourages the vector to be like a polynomial of degree $(r-1)$. A range of the vector to be "smoothed" can be specified (and if not, the smoother is applied to the entire vector). However, this restriction must be specified by an index of the vector and must be between 1 and the length of the vector, inclusive.

	\item \texttt{vector\_smoothing}\label{sec:AdditionalPrior-VectorSmoothing}

	This prior can be applied to a vector parameter. Square of (mean(vector)-k), or of (mean(log(vector))-l), or of (log(mean(vector)/m)). Restricts the vector to average arithmetically to k or m, or geometrically to exp(l). Typically used for YCS with k=1 or m=1 or l=0, to restrict the YCS to centre on 1. Optionally, indices can be chosen or excluded outside a given set of bounds.

	\item\texttt{lognormal} with mean $\mu$ and c.v. $c$\label{sec:AdditionalPrior-LogNormal}

	\begin{equation}
	- \log \left(\pi \left(p \right) \right) = \log \left( p \right) + 0.5 \left( \frac{\log \left( p / \mu \right)}{s} + \frac{s}{2} \right)^2
	\end{equation}

	\item\texttt{uniform\_log}\label{sec:AdditionalPrior-UniformLog}

	\begin{equation}
	- \log \left(\pi \left(p \right) \right) = \log \left( p \right)
	\end{equation}

	\item\texttt{element\_difference}\label{sec:AdditionalPrior-ElementDifference}

	\begin{equation}
	- \log \left(\pi \left(p_1,p_2 \right) \right) = \sum_{i = 1}^n \left( p_{1,i} - p_{2,i} \right)^2
	\end{equation}

	\item\texttt{Beta}\label{sec:AdditionalPrior-Beta}

	{Beta\index{Beta additional prior}\index{Additional Priors ! Beta} with mean $\mu$ and standard deviation $\sigma$, and range parameters $A$ and $B$, for parameter value = $p$}

	\begin{equation}
	- \log \left(\pi \left( p \right) \right) = \left( 1 - m \right) \log \left( p - A \right) + \left( 1 - n \right)\log \left( B - p \right)
	\end{equation}

	where $\nu  = \frac{\mu  - A}{B - A}$, and $\tau = \frac{\left(\mu -A \right)\left(B - \mu \right)}{\sigma ^2} - 1$ and then $m=\tau \nu$ and $n=\tau(1-\nu)$. The beta prior is undefined when $\tau \leq 0$.
\end{itemize}

Methods available for the type \texttt{vector\_average} are \subcommand{l}, \subcommand{k}, \subcommand{m}. For a target vector parameter $\textbf{X}$ and target mean $k$, the contribution to the objective score is

\begin{itemize}
	\item \subcommand{method k}

	$- \log \left(\pi \left(p \right) \right) = \left(\bar{X} - k\right)^2$

	\item \subcommand{method l}

	$- \log \left(\pi \left(p \right) \right) = \left(\overline{ln\left(X\right)} - k\right)^2$

	\item \subcommand{method m}

	$- \log \left(\pi \left(p \right) \right) = \left(ln\left(\bar{X}\right) - k\right)^2$
\end{itemize}

where $\overline{ln\left(X\right)}$ is the mean of the logged values.

There are a range of parameters and derived values that additional priors can be applied to. Here are a list of non-estimated (all parameters that can be estimated can have an additional prior attached to them) parameters that additional priors can be applied to.

\begin{itemize}
	\item \subcommand{selectivity[Selectivity\_label].values\{i:j\}}.

	This subcommand applies a selectivity to the value by age (for ages $i$ through $j$). This option is available only for certain types of selectivities (\subcommand{all\_values}, \subcommand{all\_values\_bounded}, \subcommand{double\_exponential}). See the Hoki stock assessment for an example of applying additional priors on selectivities.

	\item \subcommand{catchability[Catchability\_label].q}

	This subcommand is for catchabilities that are of type \subcommand{nuisance} only. Since \subcommand{nuisance} $q$s are not free parameters, additional priors can be applied to replicate CASAL models with \command{estimate} blocks in nuisance $q$s. If a CASAL model applied a uniform prior, then this has a null effect and this functionality can be ignored when converting to a \CNAME\ model.
\end{itemize}

This list may be useful for users who are trying to apply the equivalent CASAL penalties in a \CNAME\ model.

\subsection{\I{Parameter Transformations}\label{sec:Transformation}}
\CNAME\ has multiple methods to transform a parameter into a different \enquote{space}. Transformations are implemented to try and achieve \enquote{better} model optimisation. Complex population models can have highly correlated parameters so transforming them is a method of addressing confounded parameters, and \enquote{help} the minimisers find a \enquote{global} solution faster. To read more about transformations and get a better understanding of why they are used, see \cite{gilks1995markov}, specifically chapter 6.


To transform a parameter the \command{parameter\_transformation} block is used. For example if users wanted to estimate log \(R_0\) instead of \(R_0\), they could do the following,
{\small{\begin{verbatim}
		## define transformation
		@parameter_transformation log_R0
		type log
		parameters process[Recruitment].r0

		## define @estimate for the log parameter
		@estimate log_R0
		type uniform
		parameter parameter_transformation[log_r0].log_parameter
		lower_bound 1
		upper_bound 25
\end{verbatim}}}
%
The available parameter transformations are,
\begin{enumerate}
	\item log (Univariate transformation) Section~\ref{subsec:Transformation-types} - \ref{sec:Transformation-Log}
	\item inverse (Univariate transformation) Section~\ref{subsec:Transformation-types} - \ref{sec:Transformation-Inverse}
	\item average difference (Bivariate transformation) Section~\ref{subsec:Transformation-types} - \ref{sec:Transformation-AverageDifference}
	\item log sum (Bivariate transformation) Section~\ref{subsec:Transformation-types} - \ref{sec:Transformation-LogSum}	
	\item Orthogonal (Bivariate transformation) Section~\ref{subsec:Transformation-types} - \ref{sec:Transformation-Orthogonal}
	\item Sum to one (Bivariate transformation) Section~\ref{subsec:Transformation-types} - \ref{sec:Transformation-SumToOne}
	\item Simplex (Multivariate transformation) Section~\ref{subsec:Transformation-types} - \ref{sec:Transformation-Simplex}	
\end{enumerate}

To see the parameters that can be used in \command{estimate} block for each estimable transformation see the \subcommand{estimable parameter} description in Section~\ref{subsec:Transformation-types}.

When users estimate a transformed parameter they have the option of defining the prior for the transformed parameter or for the parameter in natural space. An example of when the later has been used. Say a meta-analysis has been done on the catchability parameter, for which an \textit{a priori} assumption can be made, but the user wants to estimate log transformed catchability for optimisation reasons. In this instance users are required to use the subcommand \subcommand{prior\_applies\_to\_restored\_parameters}. If this is true the prior will be applied to the untransformed parameter and a Jacobian will be added (if it is known) to account for the change in variable. If the Jacobian is false then the prior refers to the transformed parameter and no adjustments are needed. If users specify to calculate a Jacobian and the estimate is not a \subcommand{parameter\_transformation} \CNAME\ will print a warning and ignore this input.

\subsubsection{Transform with Jacobian}
The support of a random variable $X$ with density $p_X(x)$ is that subset of values for which it has non-zero density,
\begin{equation}
  supp(X) = \{x|p_X(x) > 0\}
\end{equation}

If $f$ is a transformation function defined on the support of $X$, then $Y = f(X)$ is a new random variable (transformed variable).

This section shows the available transformations in \CNAME\ and the resulting probability density function of $Y$. %%This theory follows the STAN manual \cite{STAN}.

Suppose $X$ is one dimensional and $f$: $supp(X) \to \mathbf{R}$ is a one-to-one, monotonic function with a differentiable inverse $f^{-1}$. Then the density of $Y$ is

\begin{equation}\label{eq:jacobian}
	p_Y(y) = p_X(f^{-1}(y)) \begin{vmatrix} \frac{\partial}{\partial y} f^{-1}(y) \end{vmatrix}
\end{equation}

where $\begin{vmatrix} \frac{\partial}{\partial y} f^{-1}(y) \end{vmatrix}$ si the Jacobian adjustment is the absolute derivative of the transform. The Jacobian measures how the scale of the transformed variable changes with respect to the underlying variable. This can be expanded to the multivariate case where the Jacobian becomes a matrix of partial derivatives.

In equation~\ref{eq:jacobian} the term $p_X(f^{-1}(y)) = p_X(X)$ and in a Bayesian context is the prior of the untransformed variable/parameter. \CNAME\ defines the objective function as the negative log-likelihood. This means \(\begin{vmatrix} \frac{\partial}{\partial y} f^{-1}(y) \end{vmatrix}\) needs to be times by a negative log, as it is currently defined as an adjustment to the density.

\subsubsection{Transformation types}\label{subsec:Transformation-types}
\begin{enumerate}
\item \subcommand{type} \subcommand{log} : natural logarithm transformation\\
\subcommand{jacobian known = true}\\
\subcommand{estimable parameter = log\_parameter}\\
$Y = log(X)$\\
$f() = log()$\\
$f^{-1}() = exp()$
\[
log \begin{vmatrix} \frac{\partial}{\partial y}  exp (y) \end{vmatrix} = log \begin{vmatrix}  exp (y) \end{vmatrix} = log(x)
\]
\label{sec:Transformation-Log}
{\small{\begin{verbatim}
@parameter_transformation log_R0
type log
parameters process[Recruitment].r0

@estimate log_R0
type uniform
parameter parameter_transformation[log_r0].log_parameter
lower_bound 1
upper_bound 25
\end{verbatim}}}


\item \subcommand{inverse}\\
\subcommand{jacobian known = true}\\
\subcommand{estimable parameter = inverse\_parameter}\\
$Y = X^{-1}$
\[
log \begin{vmatrix} \frac{\partial}{\partial y} \frac{1}{y} \end{vmatrix} = log \begin{vmatrix} y^{-2}\end{vmatrix} = -2log(y)
\]
\label{sec:Transformation-Inverse}
{\small{\begin{verbatim}
		@parameter_transformation inverse_R0
		type inverse
		parameters process[Recruitment].r0
		
		@estimate inverse_R0
		type uniform
		parameter parameter_transformation[inverse_R0].inverse_parameter
		lower_bound 0.001
		upper_bound 1
		\end{verbatim}}}
	
\item \subcommand{average\_difference} : two parameters $X_1$ and $X_2$ are transformed to $Y_1$ and $Y_2$, where $Y_1$ is the average of the original parameters and $Y_2$ is the difference between the mean and each parameter.\\
\subcommand{jacobian known = false}\\
\subcommand{estimable parameter = average\_parameter, difference\_parameter}\\
$Y_1 = \frac{X_1 + X_2}{2}$\\
$Y_2 =  (Y_1 - X_2)2 $\\
Restore transformations\\
$X_1 = Y_1 + 0.5Y_2$\\
$X_2 = X_1 - 0.5Y_2$\\
$\begin{vmatrix} \frac{\partial}{\partial y} f^{-1}(y) \end{vmatrix}$ Hasn't been assessed (i.e it could exist) \TODO{?????}\\
\label{sec:Transformation-AverageDifference}
{\small{\begin{verbatim}
		@parameter_transformation avg_diff
		type average_difference
		parameters process[InstantMortality].m{male} process[InstantMortality].m{female}
		
		@estimate avg_m
		type uniform
		parameter parameter_transformation[avg_diff].average_parameter
		lower_bound 0.01
		upper_bound 1
		
		@estimate diff_m
		type uniform
		parameter parameter_transformation[avg_diff].difference_parameter
		lower_bound -0.5
		upper_bound 0.5		
\end{verbatim}}}
	
\item \subcommand{log\_sum} : two parameters $X_1$ and $X_2$ are transformed to $Y_1$ and $Y_2$, where $Y_1$ is the natural logarithm of the sum of $X_1$ and $X_2$. $Y_2$ describes the proportion of the sum with respect to $X_1$\\
\subcommand{jacobian known = false}\\
\subcommand{estimable parameter = log\_total\_parameter, total\_proportion\_parameter}\\
$Y_1 = ln(X_1 + X_2)$\\
$Y_2 = X_1 / (X_1 + X_2)$\\
Restore transformations\\
$X_1 = exp(Y_1)Y_2$\\
$X_2 =exp(Y_1)(1 - Y_2)$\\
$\begin{vmatrix} \frac{\partial}{\partial y} f^{-1}(y) \end{vmatrix}$ Hasn't been assessed (i.e it could exist) \TODO{?????}\\
\label{sec:Transformation-LogSum}
{\small{\begin{verbatim}
		@parameter_transformation log_total_r0
		type average_difference
		parameters process[Recruitment_east].r0 process[Recruitment_west].r0
		
		@estimate log_total_r0
		type uniform
		parameter parameter_transformation[log_total_r0].log_total_parameter
		lower_bound 4
		upper_bound 25
		
		@estimate prop_r0_east
		type uniform
		parameter parameter_transformation[log_total_r0].total_proportion_parameter
		lower_bound 0.001
		upper_bound 0.8		
		\end{verbatim}}}
	
\item \subcommand{orthogonal} : two parameters $X_1$ and $X_2$ are transformed to $Y_1$ and $Y_2$, where $Y_1$ is the multiplication of $X_1$ and $X_2$. $Y_2$ is the division of $X_1$ and $X_2$\\
\subcommand{jacobian defined = true}\\
\subcommand{estimable parameter = product\_parameter, quotient\_parameter}\\
$Y_1 = X_1 X_2$\\
$Y_2 = X_1 / X_2$\\
Restore transformations\\
$X_1 = \sqrt{Y_1 Y_2}$\\
$X_2 = \sqrt{Y_1 / Y_2}$\\
$\begin{vmatrix} \frac{\partial}{\partial y} f^{-1}(y) \end{vmatrix} = 2Y_2$\\\\
\label{sec:Transformation-Orthogonal}

\item \subcommand{SumToOne} : given two parameters $X_1$ and $X_2$ that have the constraint $\sum_{i = 1}^2X_i$, estimate $X_1$ only given $X_2 = 1 - X_1$\\
\subcommand{jacobian defined = false}\\
\label{sec:Transformation-SumToOne}
{\small{\begin{verbatim}
		@parameter_transformation orthogonal_trans
		type average_difference
		parameters process[Recruitment].r0 catchability[CPUEQ].q
		
		@estimate B0_times_q
		type uniform
		parameter parameter_transformation[orthogonal_trans].product_parameter
		lower_bound 0.1
		upper_bound 2500
		
		@estimate B0_divide_q
		type uniform
		parameter parameter_transformation[orthogonal_trans].quotient_parameter
		lower_bound 0.001
		upper_bound 1e8	
		\end{verbatim}}}

% useful reference for the simplex https://mc-stan.org/docs/2_27/reference-manual/simplex-transform-section.html#fn15
\item \subcommand{simplex} : given the vector of parameters $\boldmath{X} = (X_1, \dots, X_n)$ which either has the constraint $\sum_{i = 1}^n X_i = 1$ or $\sum_{i = 1}^n X_i = n$. Then the simplex is a suitable transformation. It translates to a new vector parameter \(\boldmath{Y} = (Y_1, \dots, Y_{n - 1})\) which has unconstrained parameter space i.e \(Y_i \in (-\infty, \infty)\)\\

This transformation follows the implementation in stan, where an intermediate variable \(Z_i\) is used. The transformation going from $\boldmath{X}$ to $\boldmath{Y}$ follows\\
\[
Z_i = \frac{X_i}{1 - \sum_{j = 1}^{i - 1}X_j}
\]
and 
\[
Y_i = logit(Z_i) - log\left(\frac{1}{n -i}\right)
\]
The inverse transformation going from $\boldmath{Y}$ to $\boldmath{X}$ follows
\[
Z_i = logit^{-1}\left(Y_i +  log\left(\frac{1}{n -i}\right)\right)
\]
and
\begin{align*}
	X_i &= \left( \sum_{j = 1}^{i - 1}X_j\right)Z_i & \text{ for } i < n\\
	X_n &= 1 - \sum_{i = 1}^{n - 1} X_i & \text{ for } i = n
\end{align*}
The jacobian for the density is evaluated as follows,
\begin{align*}
|det \ J| = \prod_{i = 1}^{n - 1} Z_i \left(1 - Z_i\right) \left(1 - \sum_{j = 1}^{i - 1}X_j\right)
\end{align*}
\subcommand{jacobian defined = true}\\
\subcommand{estimable parameter = simplex}\\
\label{sec:Transformation-Simplex}
{\small{\begin{verbatim}
		@parameter_transformation simplex_ycs
		type simplex
		sum_to_one false
		parameters process[Recruitment].ycs_values{1950:2018}
		prior_applies_to_restored_parameters true
		
		@estimate simplex_ycs
		type uniform
		parameter parameter_transformation[simplex_ycs].simplex
		lower_bound -10
		upper_bound 10

		\end{verbatim}}}	
\end{enumerate}


If users want to force other parameters in the system to be the same as an estimated transformation, this can be done by creating multiple \command{parameter\_transformation} blocks. For example if there were multiple categories (spawning and non spawning males and females) and the average difference parametrisation was used to estimate natural mortality. The non-spawning components can be set the same as the spawning values using the following syntax.

{\small{\begin{verbatim}
		@categories
		format sex.maturity
		names male.spawn female.spawn male.nonspawn female.nonspawn
		
		@parameter_transformation avg_diff_spawn
		type average_difference
		parameters process[InstantMortality].m{male.spawn} process[InstantMortality].m{female.spawn}
		
		@parameter_transformation avg_diff_non_spawn
		type average_difference
		parameters process[InstantMortality].m{male.nonspawn} process[InstantMortality].m{female.nonspawn}
		
		@estimate avg_m
		type uniform
		parameter parameter_transformation[avg_diff_spawn].average_parameter
		same  parameter_transformation[avg_diff_non_spawn].average_parameter
		lower_bound 0.01
		upper_bound 1
		
		@estimate diff_m
		type uniform
		parameter parameter_transformation[avg_diff_spawn].difference_parameter
		same  parameter_transformation[avg_diff_non_spawn].average_parameter
		lower_bound -0.5
		upper_bound 0.5		
\end{verbatim}}}

This can be done for any set of parameters in the system, for example if you had multiple recruitment dynamics and wanted to estimate a joint steepness parameter with the log transformation, you would need to create multiple blocks and force them in the same.
